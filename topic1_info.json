[{"Topic":-1,"KeyBERT":["dynamics","research","quantum","models","molecular","mechanisms","model","methods","data","systems"],"LLM":["\nMolecular Simulations and Machine Learning\n\nReasoning:\nThe","","","","","","","","",""],"Representative_Docs":["The dynamics of biological systems, from proteins to cells to organisms, is complex and stochastic. To decipher their physical laws, we need to bridge between experimental observations and theoretical modeling. Thanks to progress in microscopy and tracking, there is today an abundance of experimental trajectories reflecting these dynamical laws. Inferring physical models from noisy and imperfect experimental data, however, is challenging. Because there are no inference methods that are robust and efficient, model reconstruction from experimental trajectories is a bottleneck to data-driven biophysics. I will bridge this gap by developing practical algorithms that permit robust and universal inference of stochastic dynamical models from experimental trajectories. To this aim, I will build data-efficient tools to learn stochastic differential equations and discover physical models, employing methods from statistical physics and machine learning. The main focus of SuperStoc will be in resolving models with high precision from limited trajectories. To assess the efficiency of the methods I develop, I will design information-theoretical frameworks to quantify how much can be inferred from trajectories that are short, partial and noisy. The convergence of the resulting algorithms will be backed by mathematical proofs and numerical simulations in realistic conditions. I will apply these new tools to several key open biophysical problems where existing methods are failing: condensate-mediated interactions between genomic loci, cellular mechanosensing in confined environments, pattern formation in embryo development, and visual interaction between fish leading to collective motion. The resulting algorithms will be implemented into a software designed to be useful for the broad soft biological matter community. By proving that one can do more with the same data and providing tools to do so, SuperStoc will help bridge the inference gap towards data-driven biophysics.","The present project will contribute to research in the history and philosophy of science through extensive use of state-of-the-art tools from the digital humanities. The project will focus on an investigation of recent research practice in particle physics at the European Center for Nuclear Research (CERN) in order to gain a better understanding of how knowledge is generated and validated in very large scientific collaborations. The main working hypothesis of the project is that collective research processes can be characterized, in epistemologically relevant terms, through a bird\u2019s eye view analysis of the collaboration\u2019s internal communication. The internal communication will be reconstructed from born-digital documents (e-mails, internal wiki pages, etc.) which accrue in the research practice of the collaboration. Abstracting from the case study, the project will also develop historiographic guidelines that can be transferred to future epistemological studies of modern scientific collaborations. Last but not least, the project will contribute to the philosophy of collective knowledge generation, in particular to recent issues in \u201cnetwork epistemology\u201d, by adapting the theoretical models to better fit important real-world cases. Until recently, it was nearly impossible to capture large-scale and complex research processes, such as the ones at CERN, and make them accessible for epistemological analysis. Almost all the studies of the research practice at CERN or similar cases have so far been restricted to the analysis of published articles, selected interviews and participant observation. Accompanied by historiographic guidelines and practical strategies (both of which are lacking at the moment) for best practices in the history and philosophy of science based on born-digital sources, the application of digital tools and computational methods may finally help us attain a maximally comprehensive picture of recent research practice in particle physics and beyond.","This proposal focuses on the development of new mathematical tools to analyse theoretical, numerical and modelling aspects of novel applications of nonlinear nonlocal aggregation-diffusion equations in Mathematical Biology and in classical problems of kinetic theory. Among the numerous areas of applications of kinetic modelling in Mathematical Biology, we will concentrate on phenomena identified, at the modelling stage, as systems involving a large number of \"individuals\" showing \"collective behaviour\" and how to obtain \"averaged\" information from them. Individuals behavior can be modelled via stochastic\/deterministic ODEs from which one obtains mesoscopic\/macroscopic descriptions based on mean-field PDEs leading to continuum mechanics, hydrodynamic and\/or kinetic systems. Understanding the interplay between the interaction behaviour (nonlocal, nonlinear), the diffusion (nonlinear), the transport phenomena, and the synchronization is my main mathematical goal. The proposed research is centred on developing tools underpinning the analysis of long time asymptotics, phase transitions, stability of patterns, consensus and clustering, and qualitative properties of these models. On the other hand, designing numerical schemes to accurately solve these models is key not only to understand theoretical issues but also crucial in applications. We will focus on the important case of the Landau equation with applications in weakly nonlinear plasmas by means of the gradient flow techniques. On the other hand, we showcase our tools in patterns and consensus by focusing on zebra fish patterning formation, as example of spontaneous self-organisation processes in developmental biology, and grid cells for navigation in mammals, as prototype for the synchronization of neural networks. This project connects with other areas of current interest in science and technology such as agent-based models in engineering: global optimization, clustering, and social sciences."]},{"Topic":0,"KeyBERT":["proteins","protein","cells","mechanisms","biology","biological","signaling","membrane","molecular","genes"],"LLM":["Protein mechanics and cell biology \nWords which are missing in","","","","","","","","",""],"Representative_Docs":["While advanced molecular biology approaches provide insight on the role of proteins in cellular processes, their ability to freely modify proteins and control their functions when desired is limited, hindering the achievement of a detailed understanding of the cellular functions of numerous proteins. At the same time, chemical synthesis of proteins allows for unlimited protein design, enabling the preparation of unique protein analogues that are otherwise difficult or impossible to obtain. However, effective methods to introduce these designed proteins into cells are for the most part limited to simple systems. To monitor proteins cellular functions and fates in real time, and in order to answer currently unanswerable fundamental questions about the cellular roles of proteins, the fields of protein synthesis and cellular protein manipulation must be bridged by significant advances in methods for protein delivery and real-time activation. Here, we propose to develop a general approach for enabling considerably more detailed in-cell study of uniquely modified proteins by preparing proteins having the following features: 1) traceless cell delivery unit(s), 2) an activation unit for on-demand activation of protein function in the cell, and 3) a fluorescence probe for monitoring the state and the fate of the protein. We will adopt this approach to shed light on the processes of ubiquitination and deubiquitination, which are critical cellular signals for many biological processes. We will employ our approach to study 1) the effect of inhibition of deubiquitinases in cancer. 2) Examining effect of phosphorylation on proteasomal degradation and on ubiquitin chain elongation. 3) Examining effect of covalent attachment of a known ligase ligand to a target protein on its degradation Moreover, which could trigger the development of new methods to modify the desired protein in cell by selective chemistries and so rationally promote their degradation.","Our bodies rely on protein driven shaping and remodelling of our cells\u2019 membranes to function. Uncovering the mechanisms of remodelling of the cell membrane is, therefore, essential for understanding biological processes such as fertilization, but also to allow for precise intervention in them when needed. The interplay between protein position, membrane tension, and local curvature is believed to dictate these processes. However, experimental verifications of this hypothesis in specific biological systems are scarce. Here, I propose to apply my expertise in the characterization of mechanical properties and remodelling of membranes to obtain ground-breaking quantitative details of the shaping and remodelling mechanisms in which the Tetraspanin (TSPN) family of proteins are involved. TSPNs provide an ideal case study for several reasons, they are of extreme importance to biological processes such as viral infection, they are well characterized by biochemical, genetic and proteomics approaches, and their mode of action is suspected to depend on membrane tension and curvature. Of specific interest is the role of TSPN in the formation of the newly discovered cellular organelles, called migrasomes, which are a new cell-cell communication paradigm. This proposed project addresses TSPN functions by a bottom-up approach, reconstituting the processes of interest from simple building blocks and characterizing the distinguished roles of membrane tension and curvature. To this end we will use several new assays based on combined optical tweezers, micropipette aspiration and confocal microscopy as well as AFM that will operate on crafted membrane model systems. Our unique experimental approach will allow us to recreate the conditions leading to migrasome formation, egg-sperm, and viral membrane fusion. Revealing the mechanisms underlying these processes will have direct impact on the development of infertility treatments, non-hormonal contraceptives, and novel anti-viral drugs.","The ability of cells to sense environmental cues and respond to them by adjusting their shape and motion is fundamental for biological processes ranging from animal development to disease. Much is known about how cells sense and respond to the geometry and mechanics of their environment by adhering to and pulling on the substrate. However, recent studies demonstrated that cells also strongly depend on non-adhesive interactions with the environment and that they probe, sense and deform their surroundings by pushing into them. The goal of this project is to address the mechanisms controlling cell shape and cell-substrate interactions via pushing forces. We will focus on three levels of cell organization: 1. Nanoscale pushing: We will investigate how cells locally sense and respond to obstacles without adhering to them and quantify the associated forces. Using micro-engineered substrates and tissue mimics, we will molecularly and biophysically dissect, biochemically reconstitute and theoretically model the interface between an obstacle, the plasma membrane and the actin cortex. 2. Mesoscale cell mechanics: We will investigate how actin, microtubules and intermediate filaments collaborate to generate and extend mesoscopic cell protrusions that push by adhesion-independent mechanisms. We will combine cell biological experiments and optogenetics with modeling and bottom-up reconstitutions. 3. Global force balance: We will examine adhesion-independent mechanisms that allow a cell to coordinate competing protrusions, maintain its integrity and translocate in complex environments. Using biophysical measurements and local molecular perturbations, we will test models of long-range communication within cells. Our work will provide new fundamental insights into biological and physical principles underlying the control of cell shape, integrity and motility, which are key to most physiological processes from development and homeostasis to cancer, immune responses and regeneration."]},{"Topic":1,"KeyBERT":["medieval","manuscripts","archaeological","philology","manuscript","humanities","historical","cultures","ancient","history"],"LLM":["\"Medieval Knowledge Practices & Techniques\"","","","","","","","","",""],"Representative_Docs":["Behind this project lies a paradox: water, although vital for life, has historically had little intrinsic value. Considered base and common as a drink, it was nevertheless the most symbolically charged of the elements; of limited monetary value in and of itself, access to it was in fact hard-fought, the foundation and driving force of society. The Water Cultures project aims to create a new field of study, based around the social and cultural history of a people\u2019s polyvalent interactions with water, applicable to a wide range of places and times. The project proposes an epistemological method of structuring, analysing and presenting the diverse range of findings and approaches into a new holistic understanding. The Water Cultures concept is based on the synergistic braiding of five key 'Streams': the symbolic beliefs and practices associated with water; the circulation and evolution of knowledge about water and disease and its effects; the water management systems of large cities and demands on them; the changing hydraulic landscape of rural areas; and the occupations of water, exploring the professions and trades associated with water and its delivery and uses. Italy has been chosen as the case study\u2014for its rich archives and social, political and geographical variety. The Italian history of water, from the 1500s to the end of the 19th century, is a story of authority and conflict, social hierarchy and material realities, changing medical and scientific knowledge and technological expertise, and religious beliefs and practices. Yet Italian historians have undervalued water and its uses. This transformative project conceptualises a new way of writing history, with water at its core. It proposes the history and culture of a given society, the construction of identities and forms of self-representation based on relationships and interactions with water: the ways of controlling, using and conceiving it, and the symbolic, creative and material dimensions it assumed.","From Digital to Distant Diplomatics (DiDip) will bring modern computational methods to the transregional study of late medieval charters. It will answer questions about the spread and development of pan-European documentary practices and documentary culture in the later medieval period (c. 1300-1500). The project's novelty lies in 1. the scale of the endeavour, building upon a database of over half a million digitized charters from medieval and early modern Europe (the PI's Monasterium.net) and expanding it both numerically and qualitatively and 2. the application of cutting-edge \u2018distant\u2019 macroanalysis practices (Jockers 2013) to a Europe-wide dataset, producing findings that go beyond the traditional regional and single-institution related scope of diplomatics projects. At the core of this project is the observation that the preponderance of studies in diplomatics have been and remain focused on smaller units, such as individual chanceries, collections of single institutions, or the practices of one country, language group or region (Jarret 2013) and the resignation of diplomatics scholars in the face of the large amount of documentation in the 14th and 15th century. How can we truly make an integrated study of European diplomatics in this period when it is largely addressed as a variety of hyperlocalized phenomena, without meaningful study of the relation of the practices of one area to another? The DiDip project answers this question by building a digital research environment to study the issue on a macro scale, improving the quality of research data and the methods available to enable greater breadth of study and provide findings that will point us towards a better understanding of the relationship of the various regional documentary cultures across Europe in the period.","The aim of the RELEVEN project is to develop and test new ways for digital data about historical phenomena to be created and curated so that it is most useful to historians, and to apply these methods to a methodologically challenging yet very significant aspect of medieval history. The approach is to re-frame both existing and new historical data as assertions, often sourced but always linked to an authority; this allows data to be manipulated according to source and authority, and also allows assertions themselves to be linked depending on whether they corroborate, depend on, or conflict with each other. The novel aspect of this methodology is that it takes to its logical conclusion something that historians all readily acknowledge and that is especially apparent for pre-modern history: that there are very few, if any, simple and undisputed facts. A related challenge is the contextualisation and reuse of existing online data for the period, to avoid its going to waste. The approach is tested by taking a broad trans-regional approach to the history of the late 11th century (c. 1030\u20131095), centred broadly in the eastern half of Christendom but incorporating developments elsewhere, especially in the newly Christianised kingdoms of central Europe. The looming weight of the First Crusade at the century's end means that while certain regional or proto-national narratives\u2014particularly for western Europe\u2014are well-developed, they tend to obscure the larger trans-regional trends of communication and contact, particularly in eastern Christendom. By drawing upon the depth of scholarship and the plethora of digital resources that have emerged for this period in sub-disciplines such as prosopography, textual scholarship, corpus-based research, and archaeology, and by framing this scholarship in terms of assertions whose authority is traceable, it will become possible to look at the history not just from \"the eastern perspective\", but from several."]},{"Topic":2,"KeyBERT":["neuroscience","brain","cognitive","cortex","neural","behavior","research","sensory","memory","behaviour"],"LLM":["Brain Dynamics and Information Processing","","","","","","","","",""],"Representative_Docs":["Advances in wearable displays and networked devices lead to the exciting possibility that humans can transcend the senses they were born with and learn to \u2018see\u2019 the world in radically new ways. Genuinely incorporating new signals in our sensory repertoire would transform our everyday experience, from social encounters to surgery, and advance us towards a technologically-enhanced \u2018transhuman\u2019 state. In contrast, current additions to sensory streams such as navigating with GPS are far from being incorporated into our natural perception: we interpret them effortfully, like words from a foreign menu, rather than feeling them directly. In this project, we use a ground-breaking new approach to test how new sensory signals can be incorporated into the fundamental human experience. We train participants using new immersive virtual-reality paradigms developed in our lab, which give us unprecedented speed, control and flexibility. We test what is learned by comparing different mathematical model predictions with perceptual performance. This model-based approach uniquely shows when new signals are integrated into standard sensory processing. We compare neuroimaging data with model predictions to detect integration of newly-learned signals within brain circuits processing familiar signals. We test predictions that short-term changes to normal visual input can improve adult plasticity, and measure age-changes in plasticity by testing 8- to 12-year-old children and (in another new approach, learning via wearable devices) infants. In a wide-ranging design allowing for domain-general conclusions, we work across modalities (visual, auditory, tactile) and across two fundamental perceptual problems: judging spatial layout (\u2018where\u2019 objects are) and material properties (\u2018what\u2019 they are made of). The work will provide fundamental insights into computational and brain mechanisms underlying sensory learning, and a platform for transcending the limits of human perception.","Our understanding of the neural basis of human cognition and its relation to behaviour is limited by the extent to which we can observe its underlying components. Neural activity elicited by a given stimulus can be decomposed in parallel threads of cognitive computation, each specialising on a different aspect of the stimulus. Conventional methods are fundamentally limited to tease apart these components within the stimulus-specific brain activity, therefore obscuring our understanding of the underlying mechanisms. I will build a framework to distil these threads by modelling their (trial-by-trial) distinct spatiotemporal trajectories and the interaction between them. Furthermore, I propose that the way our brains process stimuli, and in particular how these different components organise and relate to each other, can be critical to characterise subjects at the psychological and clinical level. However, it is unclear how to relate these complex models of stimulus processing to the subject phenotypes. I will develop principled algorithms to automatically discover which specific aspects of the modelled brain activity are most relevant to the traits under study. In summary, this multidisciplinary project brings together modelling and prediction across different data modalities to offer a novel temporal analytic account of how different threads of brain activity give rise to cognition, and how the nature of these elements relates to population variability. I will tackle three important questions that are representative of the addressed methodological challenges: in the study of decision-making, the relation between value representation, decision-formation and attention; in sleep research, which specific aspects of the sleep cycle are most altered in insomniacs; in the field of pain perception, the disambiguation of nociception and salience, and how these diverge in chronic pain. Despite diverse, these questions are conceptually linked by ideas presented here.","One of the most exciting yet puzzling questions in Neuroscience is how the brain coordinates the activity between different areas, integrating distinct representations into conscious percepts and thoughts. For decades neuroscientists have investigated how the brain orchestrates diverse regions\u2019 activity, pointing at oscillations as one of the key mechanisms involved in such a process. However, previous research has mainly focused on the temporal aspect of oscillatory dynamics, largely overlooking how oscillations propagate through the brain. Although rhythmic traveling waves have recently gained renewed interest, their functional role and relation to cognitive functions remain largely unknown. In this project, I will address this fundamental question: what is the role of oscillatory traveling waves in brain dynamics? I plan to take on this challenge using a multi-scale computational approach, modeling neural dynamics within and between cortical regions, as well as cortical-thalamic interactions. Importantly, the novelty of this approach consists in framing the model in the light of Predictive Coding principles, to test the compelling yet striking hypothesis that traveling waves encode Predictions and Prediction-Errors. The results of the simulations will be compared against experimental recordings in human participants to validate and assess the model\u2019s predictions. Lastly, some implementations will turn into deep learning architectures, to test their dynamics in visual tasks while improving current models of artificial vision. All in all, this proposal can significantly advance our understanding of the neurophysiological mechanisms involved in sensory and cognitive functions, testing whether and how oscillatory traveling waves are a critical mechanism in neural dynamics, and producing fundamental results in the scientific field and future technological applications."]},{"Topic":3,"KeyBERT":["superconductivity","superconducting","superconductors","quantum","entanglement","graphene","nanoscale","insulators","spectroscopy","spins"],"LLM":["'Quantum Technology Advances'","","","","","","","","",""],"Representative_Docs":["Exploring the plethora of possibilities provided by solid-state systems to realize exotic many-body phases is not only motivated by fundamental questions but also by potential quantum technological applications. In both cases, it is important to have control over the properties of the system in order to engineer the phase of interest, to have a clear theoretical understanding of the microscopic physics, and to be able to probe it. In this regard, superlattice systems have recently brought many exciting results: e.g., the moire lattice that emerges when two layers of graphene are twisted induces correlated phenomena, akin to high-temperature superconductors. Furthermore, artificially arranged atoms on surfaces have become popular tools to design electronic bands. SuperCorr will explore the vast set of possibilities provided by these tunable systems to engineer novel correlated many-body physics, propose ways to probe it, and advance our understanding of the complex phase diagrams of quantum matter. More specifically, we will address key questions related to several different graphene moire systems, such as the origin and form of superconductivity, its relation to the correlated insulator, the interplay of topological obstructions and correlations, and the microscopics of their nematic phases. We will work on the impact of spin-orbit coupling and on a theoretical description of twist-angle disorder, viewing inhomogeneities as a blessing in disguise that can also be used to probe and realize interesting physics. Finally, we will develop a theoretical framework for the design of atom arrangements on the surface of complex host materials, in order to create or simulate a quantum many-body system on demand. To this end, we will employ and further extend a variety of analytical and numerical methods of many-body physics and field theory, and combine it, in some projects, with machine-learning techniques, while keeping a close connection to experiment.","Quantum technology will revolutionize information transmission, processing, and sensing with unprecedented potential for science, economy, and the society as a whole. Yet, the strong sensitivity of quantum systems to unavoidable environmental noise impedes quantum technological breakthroughs. Here, we propose to twist coupled elemental quantum systems such that they form a global, robust quantum state that is resilient against environmental perturbations. For instance, in magnetic spin chains, fixing the magnetization at one end while rotating the magnetization at the other end can result in stable quantum helices. Such quantum twists cannot easily be unwound: They exhibit topological protection. We want to explore the full potential of this concept and extend it to higher-dimensional twists including vortices and skyrmions, see Fig. (1). The main objectives of this project are to (1) theoretically describe quantum twists in chains and arrays of atoms; (2) identify concrete realizations in cold atoms and solid state systems; (3) supply a general theory for quantum twists and connect it to topological models in high-energy physics; (4) designing and implementing an on-top error-reduction scheme for quantum information processing. The presented approach is unrelated to known quantum-mechanical topological approaches in electronic and magnetic systems that rely on momentum space, adiabatic manipulations, or globally indistinguishable quantum states. Quantum twists can serve as a topological source of entanglement, quantum energy storage, and establish an independent and versatile noise-protection mechanism for future quantum devices.","The past decade has seen remarkable advances in the field of quantum non-linear optics, where individual photons are made to strongly interact which each other. Such strong photon-photon interactions are of both fundamental and technological interest: They are the prerequisite for implementing deterministic quantum logic gate operations for processing optical quantum information. Moreover, photons that strongly interact via a quantum nonlinear medium exhibit complex out-of-equilibrium quantum dynamics that enable one to tailor and control the photon statistics of light. Quantum non-linear effects have been successfully demonstrated with few photons in a number of experimental platforms, which exploit resonant enhancement of emitter-photon coupling via high-finesse optical cavities, collective response of ensembles of strongly interacting Rydberg atoms, so-called superatoms, or efficient coupling of single quantum emitters to guided light in the realm of waveguide quantum electrodynamics (QED). However, it remains a formidable challenge to reach the true many-body regime of quantum non-linear optics, where strong interactions and entanglement between many photons and many quantum emitters give rise to exotic quantum phases of light, such as photonic molecules or fermionic subradiant states. The objective of SuperWave is to realize this regime by synergizing superatoms and waveguide QED. By uniting the expertise and experimental methods of three teams that have previously driven these fields independently, we will develop near-ideal fiber-coupled nonlinear quantum devices. Their implementation will mark a major breakthrough in quantum optics and constitute a key resource in quantum sensing, quantum metrology, quantum communication, as well as quantum simulations. We will illustrate this great potential through a number of hallmark experiments such as the coherent fragmentation of a classical light pulse into its highly nonclassical photon number components."]},{"Topic":4,"KeyBERT":["cryptography","complexity","algorithms","theoretic","modularity","computation","obfuscation","invariants","optimization","theoretical"],"LLM":["Optimization Theory, Information Networks, Secure Computation, Algebra","","","","","","","","",""],"Representative_Docs":["Motivated by applications to information networks such as wireless and ad-hoc, this research will explore fundamental informational properties of networks under communication constraints. In the standard approach, the network is assumed to be fully connected and\/or the information it carries is treated as a commodity. Here, we will investigate a markedly different paradigm where limitations on the nodes\u2019 connectivity in the network are imposed and pieces of information can be combined. Examples include storage systems in which the links are established by physical proximity or system architecture, as well as low-power wide-area networks for Internet-of-Things applications. We will consider a broad spectrum of models and topics ranging from efficient information encoding and network resiliency to information loss under corrupt nodes to cooperative models and network information accessibility. We will study these topics under specific network families and random networks. While prominently theoretic, the proposed investigation is expected to yield insights and design rules for the construction of information networks, e.g., via new data placement techniques and communication protocols. Moreover, we plan to devote part of our study to practical aspects by implementing and testing the theory developed in real-world networks. Such developments may well lead to a paradigm shift in the design of resource-limited wireless communication systems, and potentially reduce energy consumption, delays, and increase the overall information reliability. Lastly, our motivation stems from the perception that naturally occurring phenomena in general networks are dependent on the information carried by them; this perception is what drives both the research questions we pose and the mathematical model we define. Consequently, the theoretical results are expected to be practical and influential across different disciplines and research areas.","Information-theoretic secure computation is a general-purpose technique for processing sensitive data without compromising its confidentiality or integrity even in the presence of a computationally-unbounded all-powerful adversary. This notion plays an important role in cryptography, both as a stand-alone object and as a tool for computational constructs. Despite its increasing importance, we have a very limited understanding of the *intrinsic complexity* of information-theoretic security, and some of the most central feasibility questions in this area have remained open for more than three decades. In this proposal, we aim to decipher the power and limitations of this notion. We will focus on three main objectives. First, we aim to improve the complexity of general *secret sharing schemes* and exploit such improvements towards realizing highly-efficient general-purpose zero-knowledge proofs. The second objective is to explore the complexity of Secure Non-Interactive Reductions and Multiparty Randomized Encoding -- a powerful generalization of information-theoretic garbled circuits that was recently presented by the PI. The third objective is to expand our theoretical understanding of constant-round information-theoretic protocols, optimize their round complexity, and study their concrete and asymptotic computational complexity. Being part of several recent exciting developments in these areas, we believe that it is now possible to make progress in some of these basic open problems. The suggested research will bridge across different regions of computer science such as coding theory, cryptography, and computational complexity. It is expected to impact central problems in cryptography, while enriching the general landscape of theoretical computer science.","This project addresses fundamental issues in the development of algebraic topology, coarse geometry, and other areas of mathematics, related to the problem of doing algebra when the structures under considerations also have a topology. A number of other approaches have been proposed recently, showing the current importance of these issues for the mathematical community. The approach followed in this project is unique, in harnessing powerful tools from mathematical logic, and especially descriptive set theory. The fundamental idea is to enrich an algebraic object with additional information provided by a Polish cover, which is an explicit presentation of the given object as a suitable quotient of a structure endowed with a compatible Polish topology. The goal of this project is to show that fundamental invariants from homological algebra, algebraic topology, operator algebras, and coarse geometry, such as Ext, Cech cohomology, KK-theory, and coarse K-homology, can be seen as functors to the category of groups with a Polish cover. Furthermore, doing so provides invariants that are finer, richer, and more rigid than the purely algebraic ones. These invariants will allow us to tackle classifications problems for topological spaces, coarse spaces, C*-algebras, and maps, that had been so far out of reach. Furthermore, we will use these invariants to calibrate the complexity of such classification problems from the perspective of Borel complexity theory. In turn, this will enable us to isolate complexity-theoretic consequences of the Universal Coefficient Theorem for C*-algebras and of the coarse Baum-Connes Conjecture for coarse spaces, and to construct examples of strong failure of such results. Ultimately, the completion of this project will lead to the development of entirely new fields of research at the interface between logic and other areas of mathematics (algebraic topology, coarse geometry, operator algebras)."]},{"Topic":5,"KeyBERT":["electrochemical","nanostructures","heterostructures","photovoltaic","nanozeolites","electrodes","semiconductors","materials","solar","electrode"],"LLM":["Electrochemical Interfaces and Energy Storage\n\nExplanation:","","","","","","","","",""],"Representative_Docs":["Charge-transfer reactions are key not only to the way that nature fuels life in photosynthesis but also in synthesizing sustainable fuels like hydrogen. Charge transfer occurs at interfaces with an applied potential, yet almost all our understanding of electrocatalytic activity trends comes from the bulk material properties in the as-prepared state. We still lack interface-sensitive spectroscopy tools that can probe the composition and electronic structure under reaction conditions. Only with such interface-sensitive operando information can we fully understand the underlying reaction mechanisms and devise strategies for efficient energy conversion and storage. In Interfaces at Work, I will overcome these limitations by developing novel interface-sensitive operando X-ray spectroscopies combined with model electrochemical surfaces with atomic-layer compositional control, merging the fields of surface science and liquid electrochemistry. My aim is to fully visualize the physico-chemical properties of the solid\/liquid interface under operating conditions. Specifically, I will develop a new laboratory-based, multicolour operando \u201cmeniscus XPS\u201d (X-ray photoelectron spectroscopy) and transform the recently invented \u201cmembrane XPS\u201d by making it accessible to the relevant electrochemical materials using these materials themselves as new membranes. I will apply these novel techniques to electrocatalyst and pseudocapacitor model systems based on epitaxial oxide thin films and 2D carbides. Ultimately, the proposed approach will allow me to track the surface and subsurface properties under applied potential to shed light on the electrochemical mechanisms. The operando insights will result in design rules for efficient energy conversion and storage on the chemical and electronic properties of a true electrochemically active surface under operating conditions rather than the as-prepared bulk. This will help our transition towards sustainability.","To satisfy our growing energy demand while reducing reliance on fossil fuels, a switch to renewable energy sources is vital. The intermittent nature of the latter means innovations in energy storage technology is a key grand challenge. Cost and sustainability issues currently limit the widespread use of electrochemical energy storage technologies, such as lithium ion and redox flow batteries. As the scale for energy storage is simply enormous, the only option is to look for abundant materials. However, compounds that fulfil the extensive requirements entailed at low cost has yet to be reported. While it is possible that the holy grail of energy storage will be found, for example by advanced computational tools and machine learning to design \u201cperfect\u201d abundant molecules, a more flexible, innovative solution to sustainable and cost-effective large-scale energy storage is required. Bi3BoostFlowBat will develop game changing strategies to widen the choice of compounds utilizable for batteries to simultaneously satisfy the requirements for low cost, optimal redox potentials, high solubility and stability in all conditions. The aim of this project is to develop cost-efficient batteries by using solid boosters and by eliminating cross over. Two approaches will be pursued for cross-over elimination 1) bio-inspired polymer batteries, where cross-over of solubilized polymers is prevented by size-exclusion membranes and 2) biphasic emulsion flow batteries, where redox species are transferred to oil phase droplets upon charge. Third research direction focuses on systems to maintain a pH gradient, to allow operation of differential pH systems to improve the cell voltages. Limits of different approaches will be explored by taking an electrochemical engineering approach to model the performance of different systems and by validating the models experimentally. This work will chart the route towards the future third generation battery technologies for the large-scale energy storage.","This project aims to deliver a step change in our understanding of electrode and catalyst interfaces, by pioneering operando measurement capabilities that can reveal the chemistry and structure of functional interfaces under working conditions in liquid and gas environments at atmospheric-pressures and above. We will exploit enclosed reaction cells sealed with X-ray, electron and neutron transparent windows, extending their operation to conditions of temperature and pressure (up to 500 \u00b0C, 10 bar) where industrial catalytic reactions occur, as well as the liquid environments of electrochemical energy storage. These cells will be portable across complementary characterisation tools to reveal the chemical and structural evolution of material interfaces during operation. Solid-liquid studies will focus on electrode materials for Li-ion batteries, that are critical to energy storage for a low carbon economy. This will reveal the degradation mechanism that lead to capacity fade in Ni-rich cathode materials across varying conditions of stress (T, voltage, rate) during electrochemical cycling. Solid-gas studies will focus on heterogeneous catalysts for sustainable production of useful chemical feedstocks from environmentally harmful waste streams. We aim to reveal the nature of the active site in Cu-ZnO catalysts used for methanol synthesis from carbon dioxide, and understand how combining these catalysts with oxide supports influences their activity and selectivity. Relationships will be established between the interfacial structure and function of these materials in terms of their electrochemical cycling performance and catalytic activity\/selectivity. This will ultimately inform the design of new functional materials for use in technologies that are critical to a sustainable economy. The scope for research problems that can benefit from this atmospheric pressure operando approach is vast, providing many future research opportunities."]},{"Topic":6,"KeyBERT":["democracies","democracy","political","politics","electoral","democratic","deterrence","conflict","protests","partisan"],"LLM":["ELECT, WarEffects, Democratic Failure, FARRIO","","","","","","","","",""],"Representative_Docs":["WarEffects aims to advance a systematic, nuanced, and rigorous understanding of how civil wars affect women\u2019s social and political empowerment at the local level. Recent quantitative research suggests that civil wars promote women\u2019s political representation, but these accounts reflect country-level aggregate measures and often focus on a minority of political \u2018elite\u2019 women. Thus, they do not inform us how subnational and individual-level variation in civil war experiences affect the majority of \u2018non-elite\u2019 women at the local level. To address this challenge, I propose a novel theoretical framework that simultaneously explores the effects of civil wars on i) multiple dimensions of women\u2019s empowerment in the household and family, the community, and local politics. Moreover, I introduce ii) nuanced definitions for different types of exposure to civil wars, iii) the difference between changes in gender roles and gender attitudes, and iv) the moderating effect of context conditions. Building on the variation of each of these four dimensions allows me to generate a large set of hypotheses to advance a systematic and nuanced understanding of when, why, and how civil wars promote women\u2019s empowerment, and when they do not. To empirically explore these hypotheses, I will combine novel quantitative survey experiments and qualitative research in Colombia, DR Congo, and Sri Lanka. While each country case has experienced several decades of civil war, there is significant within-case and between-case variation in social context, conflict dimensions, patterns of violence, and conflict status, rendering them ideal for exploring the local effects of civil war violence on women\u2019s empowerment. Drawing on this comparative design will allow me to make statements about common patterns, divergences, and conditional effects. Altogether, this wealth of findings will establish a new conceptual and empirical research platform on the impact of civil wars on gender relations.","This project is about the elite coalition strategies that will avoid democratic failure and deconsolidation \u2013 the loss of legitimacy among its elites and citizens. It will answer the question of which government coalitions help to avoid democratic deconsolidation, and ultimately democratic breakdown. It will do so by investigating the fate of democracies in interwar Europe, the best available comparison cases for contemporary democracies, and derive precise recommendations for policy-makers today. Presently, democracy is under threat around the world. Citizens disenchanted by economic inequality and migration are losing faith in democratic regimes. Populist parties and extremists at both ends of the political spectrum threaten democratic actors, institutions, and norms throughout the Western world. Yet few if any democracies today have failed outright, and this project adopts a historical-comparative approach and investigates the fate of 24 interwar democracies in Europe. The project makes five central contributions that move it beyond the current state of the art in political science. First, it innovates conceptually by investigating democratic deconsolidation, the loss of legitimacy among democracy\u2019s elites and citizens, and democratic survival. Second, it innovates theoretically by developing a theory of strategic interdependencies between prodemocracy and antidemocracy political actors against the backdrop of citizen anger. Third, it innovates empirically by collecting new data on government coalitions and elite power grabs as well as citizen protests and political violence events. Fourth, it innovates methodologically by combining the strengths of quantitative, qualitative, and causal inference methods to investigate the effect of coalition governments with and without antidemocracy elites on democratic deconsolidation. Fifth, it innovates in terms of impact by matching historical to contemporary cases to derive lessons for today\u2019s decision-makers.","The rise of the far right poses a profound challenge to global politics. Diverse far-right actors, such as political parties, civil society groups, and social movements, have been gaining support in domestic contexts, while intensifying their transnational contacts. As these groups focus on national sovereignty and share a stance against globalization, they often contest international organizations (IOs) and their policies. Yet their impact on international organizations differs: On the one hand, far-right groups have profoundly changed negotiations on the Global Compact for Migration in the UN. On the other hand, radical-right parties in the European Parliament have hardly brought about any deeper policy changes. Why does transnational far-right contestation have varying effects on international organizations? While scholars have analyzed far-right actors in domestic politics, knowledge about their transnational activities and effects is limited. FARRIO fills this gap empirically, theoretically, and methodologically. Empirically, it compares effects of far-right contestation on the EU, the UN, and its specialized agencies\/treaties in four central policy fields (migration, women's rights, climate change, and public health). Theoretically, it proposes that IO changes depend on the directness of far-right strategies and the liberal character of international organizations. It thereby breaks new ground in identifying scope conditions for far-right impact highly relevant for research on transnational protest as well as IO resilience and change. Methodologically, FARRIO draws on and further develops quantitative and qualitative methods. It adapts protest event and networks analysis to map transnational far-right contestation, also including social media data. Bridging Comparative Politics, Social Movement Studies, and the study of International Relations, FARRIO assesses the challenge far-right actors pose to IOs as well as what measures are suited to respond to it."]},{"Topic":7,"KeyBERT":["lhc","quarkonium","hadron","qcd","collider","neutrino","positron","physics","isotopes","boson"],"LLM":["Particle Trigger Systems for LLP Detection\n\nExplanation:","","","","","","","","",""],"Representative_Docs":["The discovery of the Higgs boson at the Large Hadron Collider (LHC) closes a central chapter of the standard model (SM) of particle physics while raising several questions, such as the nature of dark matter, an explanation to neutrino masses, or the origin of baryon asymmetry in the Universe. The answer to those questions could be linked to the production of beyond the SM (BSM) particles which may have long lifetimes, compared to SM particles at the weak scale. If these long-lived particles (LLPs) were to be produced at the LHC, they would yield non-standard signatures which require dedicated identification algorithms. A complex filtering (trigger) system running sophisticated algorithms allows to decide, in real time, whether a given event of interest should be saved for data analysis or discarded. The general goal of this proposal is to enhance the trigger capabilities to enable the discovery of LLPs and thus find evidence of BSM physics exploring innovative technologies that may be of use in future facilities. With several years before the start of the High-Luminosity LHC (HL-LHC), it is now the perfect time to explore alternative trigger architectures and technologies not considered in the plans of the collaboration and that could not be explored otherwise. To this end, I will use a multidisciplinary approach involving advanced Machine Learning techniques and top-of-the-line ultra-fast processing platforms to propose an innovative solution that will improve the capabilities of future trigger systems. The foreseen studies might be the only way in which LLPs can be discovered at the HL-LHC. Any manifestation of such particles will revolutionise the field of High Energy Physics and help to answer several fundamental questions regarding the energy scale and nature of the BSM physics. Beside progressing in the frontiers of science, the designed techniques can be of great use for industries requiring real-time processing of large data-volumes to extract features.","Quarkonium, the bound state of a heavy quark pair, is central to many aspects of particle and nuclear physics. Like the hydrogen atom in quantum mechanics, the discovery of the first quarkonium J\/psi was the first tangible observation that quarks are physical particles. It provided the first direct evidence of the strong interaction theory in the subatomic world -- quantum chromodynamics (QCD). It is nowadays also understood that quarkonium analogues are important in the new physics search programme at the Large Hadron Collider (LHC) at CERN and in the worldwide dark matter hunt. Quarkonium is also a powerful probe to conduct rich physics studies. Consequently, quarkonium has been measured by almost all experiments. Despite its importance, the quarkonium production mechanism in QCD remains to be understood, which places the theoretical interpretations of quarkonium data on shaky ground. The necessary ingredient to resolve the issues is to radically improve the quality and the precision of theoretical predictions. The primary goals of BOSON are to advance quarkonium studies with three objectives: 1. Pinning down the quarkonium production mechanism; 2. Advancing the precision of theoretical predictions; 3. Extracting maximal physics information from quarkonium data. To achieve them, BOSON will follow three routes: 1) the creation of an automated tool at next-to-leading order for any process involving quarkonia and elementary point particles with interfacing to general-purpose Monte Carlo event generators; 2) next-to-next-to-leading order cross section calculations for inclusive quarkonium processes; 3) phenomenological applications to particle, nuclear, and heavy-ion physics. BOSON outlines a challenging but feasible programme to advance our knowledge of the field, with positive impacts not only on the LHC community, but also on physicists working with facilities like RHIC at BNL, Belle2 at KEK, SPS at CERN and future high-energy experiments (e.g. EIC and FCC).","In the proposed research, precision measurements of rare processes involving heavy quarks and leptons will be used to search for new phenomena beyond the Standard Model, popularly known as New Physics. This research at the intensity frontier is complementary to searches at the highest achievable energies carried out at the LHC proton-proton collider. Indications of very interesting discrepancies have recently been observed by three experiments (LHCb, BaBar, and Belle) between their results and predictions of the Standard Model in certain classes of decays of B mesons, which involve leptons in the final state. The proposed project will address these issues by using large event samples collected with the Belle II detector at a new electron-positron collider, SuperKEKB. By investigating a broad range of selected rare decays of B and D, the project will attempt to provide a definite answer on the violation of Lepton Flavour Universality, one of the cornerstones of our current understanding of the interactions among the elementary particles. Based on the results of these studies, the final stages of the project will be devoted to possible explanations and to studies of transitions that would be based on related new physics phenomena. Within the proposed research programme, novel, highly advanced identification methods for charged particles will also be developed. They will be of crucial importance to suppress backgrounds arising from other, much more abundant decays in measurements of rare processes where the sensitivity to a possible contribution of New Physics is largest. The proposed research will strongly benefit from the fact that the same group that contributed substantially to the physics programme, concept, design, and construction of the detector, will also carry out the development of novel analysis methods, their calibration and optimization for individual reactions."]},{"Topic":8,"KeyBERT":["catalysts","catalysis","catalytic","ligand","compounds","chemistry","catalyst","carbon","co2","molecules"],"LLM":["N2 activation and conversion\n\nThe topic is about developing new methods for activ","","","","","","","","",""],"Representative_Docs":["The activation of chemical bonds is fundamental to every chemical transformation. While reactions mediated by transition-metal catalysts are known for more than a century, the last decade witnessed spectacular developments in the emerging area of catalysis mediated by non-metallic species. Among these new chemical entities made from earth-abundant and inexpensive main-group elements, Lewis acid-base pairs and bifunctional element\/ligand systems mimicking the behavior of transition-metals rapidly revolutionized the activation modes of chemical bonds. In the project B-yond, I will establish new molecular engineering strategies and develop unprecedented main-group catalysts embedded in cage-shaped and curved molecular scaffolds. Ground-breaking molecules escaping the established structural theories will become the initial focus of my project, including the creation of non-planar B, Al, C and Si centered Lewis superacids with unmatched reactivities. I will push the frontiers of knowledge of chemical bonding by exploring unusual boron-elements bonding situations and advance the design of main-group catalysts beyond the state-of-the-art. Unprecedented C-H bond functionalization processes will be developed and exploited for hydrocarbons transformation through the concept of \u201clow reorganization energy catalysts\u201d. The activation of dinitrogen with unique main-group superacids and bases will finally be tackled, a pioneering step toward the uncharted territory of catalytic N2 activation and conversion without transition-metal complexes. These goals will be accomplished through a multidisciplinary approach built on my expertise in mechanistic investigations, spectroscopic and kinetics methods, organometallic and main group chemistry. The project B-yond will deliver extraordinary solutions for chemists to reach new chemical reactivities beyond the actual limits and will inspire scientists to develop innovative sustainable and cost-effective chemical processes.","Ammonia is one of the most important chemicals in the world. Electrocatalytic reduction of nitrogen (NRR) at ambient conditions is a sustainable alternative for its production to the established energy consuming Haber-Bosch process, relying on hydrogen from fossil sources. The triple bond in dinitrogen is one of the most stable covalent chemical bonds. Conversely, the dissociation of dinitrogen and its chemical conversion is highly demanding. NRR is a carbon-neutral and decentral process that can be carried out wherever renewable electricity, water, and air are available. However, current research on NRR and other electrocatalytic reactions has reached an impasse as improvements based on catalyst design are getting more and more incremental. At this tipping point, CILCat tackles a foreseeable stagnation by constituting a disruptive principle that holds holistic perspectives for the activation of small molecules. The novel concept will go beyond established principles of isolated catalytically active sites. By confining ionic liquid (IL) electrolytes into charged porous carbon materials, an interface will be created, that as a whole serves as catalytic surface. CILCat will contribute to a fundamental understanding of the physicochemical principles of sorption into ILs upon confinement in pores. Targeted catalyst development will follow and the possibility of using the principle for catalytic activation of nitrogen and other molecules will be explored. This innovative approach will then be combined with advanced electrode design. CILCat aims for more than a step towards a future carbon-free nitrogen economy. It is a pioneering attempt to heterogenize homogenous catalysts by rather converting the energy principles of small molecule activation than chemical structures from solution to surfaces. The methodology is transferable to other obstacles in the field of catalysis and the project will lead to a more objective general understanding of reactivity in confined spaces.","The grand challenge for the chemical industries of the 21st century is the transition to more sustainable manufacturing processes that efficiently use raw materials and eliminate waste. Catalysis engineering is the key enabling technology to drive this transition, and single-atom catalysis is an emerging new approach to catalyst design. However, major questions concerning the local structure of these systems, their reactivity, and their evolution when prepared and structurally integrated into chemical devices are elusive. This project will address these important scientific gaps, laying the foundation for a new generation of catalysts for CO2 conversion. To unveil their microscale functioning, I will study for the first time the charge transfer taking place before, during, and after reactant adsorption and surface reactivity. This will be done combining synthesis, operando characterizations, microkinetics, and theoretical methods. Then, merging microreactor technology and process intensification, I will manufacture single-atom catalysts in powder and as miniaturized thin films or foams, using new, scalable and greener methods. This will bypass current limitations in terms of efficiency and metal dispersion, and close the gap on challenges related to catalyst-reactor integration, bridging chemical and device engineering. The materials will be validated in the valorization of CO2 to derive structure-function relationships and prove major catalytic improvements under realistic conditions. Overall, this is a fundamental and interdisciplinary project with ambitious objectives and high-risk\/high-gain potential, that will go beyond the traditional pillars of catalysis. The scientific outcomes will provide new perspectives in catalysis and open paths in other fields, such as materials chemistry, green synthesis, and purification science. My pioneering contributions in this field and new proof-of-concept data place me in a unique position to undertake this fundamental study."]},{"Topic":9,"KeyBERT":["nanoscale","microscopy","fluorescence","molecules","molecule","molecular","intracellular","spectroscopy","nanopore","proteins"],"LLM":["Single-molecule analysis \n\nThis label captures the essence","","","","","","","","",""],"Representative_Docs":["Reading biomolecular signatures and understanding their role in health and disease is one of the greatest scientific challenges in genome and proteome biology. Yet, complete protein analysis at the single-molecule level remains an unmet milestone. This pursuit is fundamentally hindered by the huge dynamic range of protein cell expression and the insufficient spatio-temporal resolution of current analysis methods. Next-generation single-molecule techniques that can precisely manipulate and sequence proteins in space and time are urgently needed to reach this goal. Among these, nanopore platforms are at the forefront, leading in terms of read length, throughput and sensitivity. However, the major challenges associated with translocation speed control and the precise-readout in solid-state nanopore devices, remain prohibitive. In SIMPHONICS, I will resolve these issues by developing the first integrated platform that combines nanopore transport measurements, spatially modulated acoustic wavefields and single-molecule fluorescence time traces to confine, scan and optically fingerprint proteins in a non-invasive and massively parallel manner. The feasibility of this method will be established by attaining three main objectives: 1) Confining and controllably manipulating individual molecules using acoustic nanotweezers; 2) On-demand engineering of 2D material optical emitters as ultrabright fluorescent probes for energy transfer based detection, and 3) Identifying proteins\/peptides from their optical signatures in multi-color F\u00f6rster resonance energy transfer (FRET) during acoustophoresis. With this powerful and unique platform, I will harness the vast potential of acousto-photonic interactions in monolithic nanopore devices. Successful achievement of the project objectives will result in a high-throughput and non-destructive protein fingerprinting platform and signify a considerable leap forward in our quest to unravel the human proteome.","Single Molecule Analysis in Nanoscale Reaction Chambers Imagine that you would measure the average eye color of the population in Sweden. Clearly it would not say much about the colors of the eyes of the inhabitants. To obtain this information, one must of course study them individually. The same holds true for complex biological molecules, especially proteins, which may exist in many different configurations that cannot be resolved in an ensemble measurement. Heterogeneities in biomolecular structure and function limit our understanding of biology. To advance further it is vital that we study biomolecules individually. For proteins this is highly challenging since it must be done in a non-invasive manner under conditions similar to their native environment. The SIMONANO project aims to develop a new platform for single molecule analysis which provides essential advantages. Proteins will be controllably loaded into solid nanoscale chambers, thereby eliminating the need of field gradient forces or surface immobilization. Furthermore, the proteins are entrapped at physiological conditions and small ligands can still access them quickly. Most importantly, the content is regulated on the single molecule level, i.e. proteins can be controllably loaded one at a time and different types of proteins can be introduced sequentially. Advanced (but established) fluorescence microscopy techniques will be used to detect the proteins and analyze their reactions. The possibility to reliably entrap any desired number of proteins under physiological conditions and study their reactions will provide great scientific advancements in the life sciences. Once developed in this project, the nanoscale reaction chambers can become a tool used by biologists worldwide, which will advance our understanding of life on the molecular level. This will in turn lead to new applications in biotechnology and medicine.","The ideal microscopy experiment would take place in native cells without genetic engineering, with 3-dimensional resolution on the single molecule scale (<10 nm) by observing the endogenous molecule itself. I propose the introduction and use of deuterium (\u201cdeuterON\u201d) as a general method for a multimodal approach, to i) synthesize a first-in-class deuterated silicon-containing cyanine (SiCy) fluorophore for super-resolution imaging, ii) to design and test deuterated, next-level photoswitches to restore vision, and iii) use these probes and deuterated drugs for direct and bioorthogonal, spectroscopic imaging. In particular, deuterated SiCys will allow stochastic reconstruction microscopy (STORM) by using near infra-red light to break new ground in protein localization in live tissue, opening the gates for thick sample imaging (~100 \uf06dm axial) with retained super-resolution (~20 nm). In addition, deuterated azobenzene photoswitches will be designed to finally reach the indispensable, and to-date unobtained. light sensitivity to remote control neural signalling and vision restoration in vivo. Lastly, the use of \"label-free\" labelling and imaging will be explored with deuterated drugs to observe drug uptake and metabolism by utilizing the unique properties of the carbon-deuterium bond in Raman spectroscopy. Coupled to a confocal microscope, deuterated drugs will be tracked in native and live cells, without any genetic engineering strategies, and on the molecule of interest itself, reducing perturbations and artefacts to a minimum. This ground-breaking approach holds promise to be generalizable to Chemical Biology disciplines, and as an unconventional, yet attractive and powerful method to design and synthesize next generation small molecule probes. Developing a pipeline for these aims will be a game changer, with ramifications for the life sciences, cell biology, drug development and with prospective translational impact."]},{"Topic":10,"KeyBERT":["astrophysical","supernovae","hubble","galactic","astronomy","cosmology","galaxy","telescopes","cosmological","stellar"],"LLM":["Stellar parameters determination \n\nExplanation:\nThe topic","","","","","","","","",""],"Representative_Docs":["What makes our Galaxy\u2019s ecosystem so fascinating is the complex interactions between its components: stars, gas, dust, magnetic fields, and cosmic rays. Of these components, the Galactic magnetic field (GMF) may well be the most enigmatic. Only partially observable through indirect means, its study relies heavily on modeling, almost exclusively using line-of-sight integrated radio-polarimetric data. Although much has been learned, many questions are still unanswered especially about the turbulent, small-scale field component and out-of-plane field. The crucial innovations proposed here are large independent data sets with 3D (distance) information \u2013 which can only be provided by stars polarized due to differential absorption by interstellar dust, with known distances \u2013 and more advanced Bayesian statistics which allows including prior knowledge and enables quantitative model comparison. I propose to use 2 new polarization surveys in the V (visual) band, resulting in polarimetry of millions of stars across the southern sky. With distance information provided by the GAIA satellite, this improves the current data situation by 3 orders of magnitude. We will test GMF models against all available data, employing a Bayesian inference software package which we are developing. In the process, we will produce the first 3D all-sky (out to absorption limits) dust distribution consistent with both UV\/optical\/near IR absorption and optical polarization. This research will result in a next-generation GMF model that includes all observational GMF tracers and can use informative priors. It will allow mapping out interstellar magnetized turbulence in the Galaxy, instead of providing averaged parameters only, and understanding the interplay between the local GMF, gas and dust. Its legacy is a 1000x increased stellar polarization catalog, an all-sky 3D dust model, a bayesian sampler for GMF models, and a superior GMF model for use in cosmic ray modeling or foreground subtraction.","Stellar evolution models suggest that there ought to be ~10e7 stellar-mass black holes (BHs) in our Milky Way. However, we currently know only of ~20 BHs, and those are in binaries where accretion makes them shine in X-rays. Beyond that, no non-accreting \u2018dormant\u2019 BH has ever been robustly identified across the Galactic disk. Finding dormant BHs in binaries (dBHBs) is fundamental to learning when which BHs form, how massive stars die, and what the precursors of BH gravitational wave events are. Such BHs cause characteristic time variations in radial velocity (RV), flux, and light-centroid positioning of their luminous companion, providing an avenue for detection and study. Spectroscopic, astrometric and photometric surveys now yield the data needed to search for dBHBs. Yet, recent dBHB candidates have instead turned out to be short-lived evolutionary phases of close binary stars: thus, any successful search for dBHBs must entail sifting through vast samples using a combination of these signatures, and rigorously eliminating \u2018false positives\u2019. I propose an unprecedented search for Galactic dBHBs that should find ~100 of them, drawing crucially on the spectra of ~580,000 massive stars from the SDSS-V survey. As SDSS-V project scientist, I have helped shape it as the only all-sky, multi-epoch spectroscopic survey, systematically focused on stellar physics. Novel analysis of these spectra will be meshed with detailed modeling of TESS light curves and Gaia astrometry. Through guaranteed-time, high-resolution follow-up spectroscopy on the candidates, we will get detailed RV curves and crucial \u2018spectroscopic disentangling\u2019 to identify false positives that have two luminous components. Either dBHBs do not exist in any numbers in our Galaxy, or this proposal will find and characterize them. Beyond the \u2018risky\u2019 search for dBHBs, this program will break ground in identifying numerous other dark companions to massive stars, such as white dwarfs or neutron stars.","Fundamental stellar parameters are the primary data required for an in-depth understanding of stars, their interiors, and their environments. With the progress of stellar physics and the prospects of ground facilities or space missions, it is critical to improve the accuracy, and quantity of such data. The development of exoplanet and asteroseismology domains is demanding direct data to eliminate any bias in the parameters. And finally, the differences between methods for determining the Hubble constant is motivating new and precise direct determination of distances on the primary candles of the cosmic scales. Many methods like asteroseismology, photometric transits of exoplanets, radial velocities, or Surface Brightness Colour (SBC) relations are linked to the stellar radius. Usually estimated through models, its determination by coupling an optical interferometric measurement of the angular diameter and, for example, a Gaia parallax, is the best way to avoid any model dependence. Furthermore, characterizing any activity (limb darkening, convection, rotation, spots, or binarity) is also mandatory, both for bias removal and for the required progress on stellar physics. Through an ambitious and homogenous survey of the angular diameters of a thousand stars as faint as magnitude 8 in the visible and as small as 0.2 milliseconds of arc, my project is built to address key questions about the relation between planets and stars and to offer to the broader community a unique and primary source of direct information on a representative and large sample of stars all over the HR diagram. A few hundred of direct measurements of limb darkening will be accessible, and, for about 100 stars, activity characterization with a more detailed surface imaging will be possible. These data will also permit the development of new unbiased SBC relations to serve the faint targets of space missions like PLATO or ARIEL in the future and the distance scale within the Araucaria project."]},{"Topic":11,"KeyBERT":["ecology","ecosystems","ecosystem","ecological","biodiversity","biogeochemical","conservation","trophic","grassland","microbiomes"],"LLM":["Ecological Dynamics Under Global Change and Restoration","","","","","","","","",""],"Representative_Docs":["Global change degrades ecosystems worldwide. To mitigate its effects is the environmental challenge of our age, and restoration has emerged as the main strategy to stem the biodiversity crisis and repair damaged ecosystems. Despite substantial progress on the number of restoration studies and datasets, there is a fundamental gap in our understanding and prediction of the patterns and mechanisms underlying ecological restoration and how they are altered by global change. The goal of RECODYN is to determine the recovery rates and trajectories of biodiversity, community structure and ecosystem functioning in complex multitrophic communities, and how climate change and habitat fragmentation \u2013 two of the largest threats to biodiversity and ecosystems in terrestrial systems \u2013 influence those dynamics. To achieve this, I will use an integrative approach that combines the development of new theory on metacommunities and temperature-dependent food web dynamics in close dialogue with a unique long-term terrestrial mesocosm experiment. RECODYN is articulated around three objectives. First, I will investigate differences between natural assembly and recovery dynamics. Then, I will determine the effects of global change \u2013 i.e. climate change and fragmentation \u2013 on biodiversity, community structure, spatial and temporal stability, and key ecosystem functions of recovering ecosystems. Finally, I will provide creative solutions to restore ecosystems in a warmer and more fragmented world. RECODYN proposes an ambitious integrative and innovative research program that will provide a much-needed new perspective on ecological restoration in an era of global change. It will greatly contribute to bridging the gap between theoretical and empirical ecology, and to move restoration from an idiosyncratic discipline to a more predictive science. RECODYN will foster links with environmental policy by providing new restoration measures that derive from our theoretical and empirical findings.","Life on Earth, as we have known it for millennia, is at stake. Human activities are putting all kinds of ecosystems under increased stress because of land-use change and the alteration of the biogeochemical cycles of nitrogen (N), phosphorus (P) and carbon (C), thus inducing climate warming. Functionally diverse ecosystems are more productive and stable than less diverse ones, and biogeochemical changes affect both biodiversity and the elemental composition of organisms (their elementome), changing how they and their ecosystems function. It is, therefore, imperative to provide evidence about how the interactions between elementomes, biodiversity, and climate drive ecosystem functioning if we are to avoid the serious threat of reducing essential resources for life within the context of global change. STOIKOS will achieve an in-depth understanding of the interaction between elementomes and biodiversity in determining ecosystem functioning by introducing the concept of elemental diversity, and moving functional ecology from using functional traits to elementomes, an easy and universal way to compare all sort of organisms. STOIKOS will particularly test the hypothesis that community-weighted elementomes and elemental diversity explain ecosystem functioning better than functional traits and their diversity. STOIKOS will integrate data from observations (field campaigns), long-term monitoring sites, microcosm experiments and theoretical modelling to provide synergies amongst their outputs to build the foundations of an elemental-based ecology. This will allow STOIKOS\u2019 hypotheses to be tested at the individual, species and community\/ecosystem scales using new and game-changing methodologies and study systems. The cutting-edge science of STOIKOS will not only provide the foundations of an elemental-based ecology, but will also deliver new ecological theory and methodological tools that will help us predict the future of ecosystems and assess the fragility of our biosphere.","Life on Earth, as we have known it for millennia, is at stake. Human activities are putting all kinds of ecosystems under increased stress because of land-use change and the alteration of the biogeochemical cycles of nitrogen (N), phosphorus (P) and carbon (C), thus inducing climate warming. Functionally diverse ecosystems are more productive and stable than less diverse ones, and biogeochemical changes affect both biodiversity and the elemental composition of organisms (their elementome), changing how they and their ecosystems function. It is, therefore, imperative to provide evidence about how the interactions between elementomes, biodiversity, and climate drive ecosystem functioning if we are to avoid the serious threat of reducing essential resources for life within the context of global change. STOIKOS will achieve an in-depth understanding of the interaction between elementomes and biodiversity in determining ecosystem functioning by introducing the concept of elemental diversity, and moving functional ecology from using functional traits to elementomes, an easy and universal way to compare all sort of organisms. STOIKOS will particularly test the hypothesis that community-weighted elementomes and elemental diversity explain ecosystem functioning better than functional traits and their diversity. STOIKOS will integrate data from observations (field campaigns), long-term monitoring sites, microcosm experiments and theoretical modelling to provide synergies amongst their outputs to build the foundations of an elemental-based ecology. This will allow STOIKOS\u2019 hypotheses to be tested at the individual, species and community\/ecosystem scales using new and game-changing methodologies and study systems. The cutting-edge science of STOIKOS will not only provide the foundations of an elemental-based ecology, but will also deliver new ecological theory and methodological tools that will help us predict the future of ecosystems and assess the fragility of our biosphere."]},{"Topic":12,"KeyBERT":["herding","cooperation","societies","coalitional","exploitation","cooperative","normativity","systematic","analysis","pastoral"],"LLM":["'Honor and Cooperation'","","","","","","","","",""],"Representative_Docs":["Few geographical spaces have been more relevant to human life and more intensively theorized than the Mediterranean Sea. Today, this sea poses some of the most pressing challenges and opportunities for European economic, security, and environmental policies. Answers to how to manage the region depend on ideas and perceptions of integration and division of the basin and its peoples. But the Mediterranean as a spatial concept has radically changed in the last 160 years as humans have gained access to its depths, unveiling an underwater world to discover, exploit, and navigate. The Mediterranean has become a volume. DEEPMED is the first historical account of the discovery of the deep Mediterranean environment. Its main hypothesis is that science and strategy jointly made the Mediterranean depths into an object of analysis and a political space, which in turn shaped science and strategy in the region. DEEPMED pursues three specific objectives: 1) identifying the actors and contexts that enabled perceptions and practices of depth in the Med; 2) describing how natural and human time-scales interact in this body of water, and 3) tracking key conceptual landmarks defining the uniqueness and representativeness of the Mediterranean volume vis \u00e0 vis the global ocean. DEEPMED is the first basin-scale step in a novel approach to oceanic history that incorporates analyses of deep and bottom layers of the Sea to gauge the causes and effects of the historical emergence of depth. This requires an innovative interdisciplinary, transnational and digital methodology. The project identifies overarching trajectories of human engagement with Mediterranean depths from the mid-19th century to the present, including contrasting timelines and perspectives. The availability of digital tools for creating a database that facilitates geospatial and visual analyses make the project timely. The current security and environmental Mediterranean crises make it essential.","The distinction between agents and non-agents is central in law: only agents can decide about their affairs, enter contracts or be held responsible for their actions. Western law has since the 19th century relied on an understanding of agency that has developed in the liberal tradition of thought. According to this \u201cLiberal Agency\u201d, agents are highly rational human individuals. Persons with cognitive disabilities, artificial intelligences, and nonhuman animals are therefore not agents because of their lack of rationality or humanity. This view of agency is deeply embedded in Western legal systems. Liberal Agency has recently come under increasing criticism and challenges. Many now argue that persons with disabilities, artificial intelligences and\/or nonhuman animals could in fact be treated as agents. However, these criticisms have been narrow in scope, and the debate is overall highly fragmented. LEGACY, situated within legal philosophy and history, will offer a comprehensive reappraisal of legal agency. It will investigate three main questions: First, how did Liberal Agency become the dominant understanding of agency in law? Second, what challenges confront Liberal Agency today? And finally, what kind of a theory can best explain the evolving notion of agency in law? The objectives of the project are to deliver: (1) a systematic understanding of the historical background and context of Liberal Agency, its spread in Western legal thought, and its challengers; (2) an in-depth analysis of contemporary agency accounts that incorporates and conceptualizes the historical insights with a systematic understanding of the contemporary debates; and (3) a theory of agency that provides an overall synthesis of the accounts and can explain, reconcile and\/or solve the outlined contemporary challenges. The project will thus develop a historically informed, comprehensive and rigorous understanding of legal agency, based on a broad synthesis of legal and philosophical thought.","Understanding (un)willingness to coordinate with others, to compromise when faced with different choices, or to apologize for transgressions is crucial as these behaviors can act as strong facilitators or inhibitors of important interpersonal processes such as negotiations and coalition building. These behaviors play a major role when individuals from different cultural backgrounds work together to solve disputes or address joint challenges. Yet, we know little about what these behaviors mean in different cultural groups or how they are approached. With HONORLOGIC, I aim to initiate a step-change in our understanding of cultural variation in these important domains of social behavior by providing unique, multimethod, comparative and converging evidence from a wide range of cultural groups. I will answer the question \u201cHow do cultural groups that promote honor as a core cultural value approach coordinating with others, reaching compromise, and offering apologies?\u201d by integrating insights from social\/cultural psychology, behavioral economics, and anthropology. I will do this by collecting quantitative data using economic games, experiments, and surveys from Spain, Italy, Greece, Turkey, Cyprus, Lebanon, Egypt and Tunisia, as cultural groups where honor has been shown to play a defining role in individuals\u2019 social worlds. I will also run the proposed studies in the US, the UK, Japan and Korea to provide a broader comparative perspective. HONORLOGIC will produce transformative evidence for theories of social interaction and decision making in psychology, economics, and evolutionary science by (a) producing innovative theory and data with an interdisciplinary and multi-method approach, (b) increasing the diversity of the existing evidence pool, (c) testing established theoretical assumptions in new cultural groups, and (d) contributing to capacity building in under-researched cultural groups in psychological research."]},{"Topic":13,"KeyBERT":["extratropical","arctic","convective","tropics","glaciers","forecasts","climate","atmospheric","subseasonal","clouds"],"LLM":["Atmospheric dynamics, Arctic amplification, clouds, climate surprises","","","","","","","","",""],"Representative_Docs":["We expect that temperatures over the wintertime central Arctic will increase by 20 degrees - and precipitation will double - by the end of this century if greenhouse gas emissions continue to rise. Arctic sea-ice is projected to completely melt in summer within the next decades, and may cease to form in winter in the coming century. The traditional framework to understand this Arctic amplification of climate change focuses on the steady-state mean Arctic climate. However, the Arctic wintertime atmosphere has two preferred states that are largely controlled by initially warm and moist air masses that cool and dry after being advected from lower latitudes. We understand little about how these air masses cool and dry, and what controls the sudden transition from a cloudy state to a clear state along their trajectory. This lack of understanding is a major obstacle to scientific progress and improved climate models. To achieve groundbreaking progress, I will analyze the warm, moist poleward flows, cold, dry equatorward flows and the air-mass transformations that lead from one to the other. I will observe and model such air masses along their trajectories using recently developed air-mass following balloons and customized model setups. Cooling of air in the Arctic mirrors heating in the Tropics. Together, these drive the global atmospheric circulation, but the Arctic\u2019s role in this picture has largely been overlooked. My team will investigate how the Arctic couples to the global climate system using a novel concept of averaging the atmospheric circulation. We will focus on how and why both the heat and moisture content and the amount of air transported into and out of the Arctic change in a warming world and contribute and respond to Arctic amplification. A3M-transform will deliver a step change in understanding the air-mass transformation processes that shape Arctic amplification and transform our view of how the Arctic couples to the global climate system.","The ways in which clouds change with global warming remain elusive, as are the associated cloud-climate feedbacks that govern most of the spread in climate sensitivity simulated by current Earth System Models. This uncertainty in turn limits society's ability to take necessary action to avoid dangerous climate change. Despite considerable research progress in recent decades, additional complexities have been uncovered that further add to the uncertainty. For example, the understanding that many cloud-climate feedbacks change with time, due to their dependence on warming levels or patterns, is relatively recent. Cloud thermodynamic phase changes are the root cause of some of this state-dependence, and new research has revealed that these feedbacks could shift Earth's climate into a state that is more sensitive to greenhouse gas forcing than at present. Understanding and quantifying this state-dependence is therefore critically important, but such progress will require deep understanding of processes on a range of scales, from the microphysics that control cloud phase to large-scale impacts on climate. Furthermore, it has become evident that different cloud-climate feedback regimes are governed by different processes with their own unique state dependence that must be investigated separately. Therefore, the overall objective of STEP-CHANGE is to understand and quantify feedbacks associated with cloud phase changes, including their state-dependence, for three distinct cloud regimes in the following regions: the Arctic, the Tropical deep convective region, and the Southern Hemisphere storm tracks. This will be achieved through a bold and innovative research strategy which includes aircraft measurements, lab experiments, space-borne remote sensing, and a hierarchy of numerical model simulations. STEP-CHANGE builds on recent discoveries and innovations within the PIs research group, and is motivated by key knowledge gaps identified in recent IPCC assessment reports.","The Greenland and Antarctic ice sheets (GrIS and AIS, respectively) and the Atlantic Meridional Overturning Circulation (AMOC) are prominent examples of tipping elements in the Earth system that have the potential to respond nonlinearly to small changes in forcing. Tipping elements can thus give rise to climate surprises, i.e., low-probability, high-impact events that may be triggered earlier than expected. Simulating such climate surprises and their impacts, on the relevant multi-centennial timescales and beyond, is particularly challenging. Today, the right methods are not available, resulting in deep uncertainty in future projections. Here I aim to develop a novel, probabilistic methodology to robustly forecast climate surprises such as ice-sheet and AMOC collapse on long timescales. This requires simultaneous advances beyond the state of the art on two fronts. First, a new generation Fast Earth System Model (FESM) will leverage the latest advances in our understanding of key processes to represent the GrIS, AIS and AMOC realistically, in a coupled framework and on long timescales. Critically, this will be the first comprehensive model fast enough to run the large ensembles of simulations needed to quantify the uncertainty associated with deeply uncertain processes. Second, a highly novel and generalized probabilistic approach will be developed, to constrain the FESM to be consistent with output from the latest generation of Earth System Models. FORCLIMA will generate probabilistic estimates of climate surprises for the medium-term future (centuries to millennia) with much higher confidence than we have today, and inform about interactions between key tipping elements in the climate system. This project will therefore greatly advance the state of the art in coupled climate \u2013 ice-sheet modeling, and lead to an unparalleled understanding of the long-term impacts of climate change on the Earth system."]},{"Topic":14,"KeyBERT":["crustal","tectonics","meteorites","geology","mantle","chromite","magmas","earth","planets","isotopic"],"LLM":["Earth's inner core and planetary atmospheres \n\n","","","","","","","","",""],"Representative_Docs":["Understanding Earth's crustal growth is crucial to understanding the evolution of its tectonics, the birth of the first continents, and the fundamental changes that transformed Earth into a habitable planet. However, much of our understanding of Earth's crustal growth is predicated on a single mineral - zircon - that is strongly biased towards detecting felsic crustal growth. This is particularly problematic for the early Earth, where average crustal compositions were far more mafic than today, and the very first protocrust may be entirely undetectable using conventional methods. I propose to access the mafic to ultramafic crustal growth record using detrital chromite preserved in sedimentary rocks from Archaean Cratons. Like zircon, chromite chemical compositions reflect the magmas that they crystallised from, and can be used to identify the provenance of the mafic portions of a sedimentary rock. Furthermore, they can be dated using Re-Os isotopes, to identify the age of eroded mafic terranes. This project has three main objectives: 1. Develop techniques to identify the age and composition of chromite sources in ancient sedimentary rocks. 2. Use a range of detrital chromite samples from sedimentary sequences in the Superior Craton, to reconstruct a mafic-ultramafic crustal growth curve for the craton. 3. Search for evidence of Earth's mafic protocrust in some of the oldest known chromite-bearing sedimentary rocks. These achievements will open a new avenue for studying sedimentary provenance, unlock the archive of mafic crustal growth throughout Earth history, and provide insights on the nature and survival of Earth's earliest crust. The techniques developed will be broadly applicable, paving the way for a better understanding of Earth's crustal evolution.","The low density of the Earth\u2019s continental crust has been proposed to be at the origin of plate tectonics. Physical studies on the continental crust have shown that its low density was acquired by differentiation of the crust and loss of dense mafic residues. What is the composition and vertical structure of the crusts of the other terrestrial planets, which do not show plate tectonics? How did they form and what are the modifications they have undergone following their formations? Are they far from being of continental-type? To answer these questions, I propose to study from a physical perspective the crust structure and the processes of crust formation and evolution on terrestrial planets other than Earth using innovative thermal, mechanical and dynamical models combined with new planetary observations. Temperature is a crucial control variable as it dictates phase changes, buoyancy, mechanical properties and stress state. In the crust and stagnant lithosphere of terrestrial planets, temperature is controlled by the distribution of heat producing elements. Lithosphere cooling being the most likely cause of quakes on stagnant-lid planets, we propose to constrain the concentration and distribution of heat producing elements on Mars and the Moon by comparing recorded and predicted seismicity from thermal evolution models. From these thermal evolution models, we will also evaluate the potential for planetary crust differentiation and evolution. From magma ascent models, sensitive to crust density and mechanical state, combined with systematic in quantitative observations of volcanic structures and deposits on terrestrial planets, we will constrain the crust structure and thermal state. Finally, we will develop new models of primitive crust formation in a stagnant lid regime of convection to evaluate the characteristics of primitive crusts on terrestrial bodies.","Planetary atmospheres are fundamental reservoirs controlling the habitability of planets. The chemical and isotopic compositions of atmospheric constituents also hold clues on the geological evolution of the entire planetary body. Today, Earth's atmosphere contains about 80% dinitrogen and 20% dioxygen. Yet, there is no scientific consensus on how and why these two molecules emerged and persisted in the Earth's atmosphere. The interactions between the atmosphere and the continental crust also play a major role in controlling the bio-availability of nutrients and the composition of the atmosphere, and thus the climate. However, the evolution of the volume of continental crust over time is strongly debated. Project ATTRACTE will significantly improve our knowledge of the main drivers of atmospheric evolution over time. This will be achieved by going back in time and following the evolution of the composition of the Earth's atmosphere over geological eons. Analyses of gases contained in traditional and new paleo-atmospheric proxies, the post-impact hydrothermal minerals, will be carried out with innovative mass spectrometry techniques. The isotopic composition of paleo-atmospheric xenon will provide new constraints on the history of hydrogen escape for the Archean Earth. Coupled argon and nitrogen measurements will allow to determine, for the first time, the evolution of the partial pressure of atmospheric dinitrogen. Paleo-atmospheric data gathered during the project will be fed in numerical models of Earth's atmospheric and crustal evolution. This will allow to reconstruct how volatile elements have been exchanged between the silicate Earth and the atmosphere through time. Results gathered during project ATTRACTE will ultimately provide new datasets for climate studies of the ancient Earth but will also help building the scientific framework required to interpret future observations of exoplanetary atmospheres and to portray the geology of extrasolar planets."]},{"Topic":15,"KeyBERT":["stakeholders","research","governments","conservation","china","consortia","collaborative","organisational","project","analysis"],"LLM":["Challenging power in data-driven decision making\nThe long description given","","","","","","","","",""],"Representative_Docs":["Collaborative planning has become an effective means to address conflicts of interest in urban renewal and environmental management in China. However, the egalitarian principles that ground collaborative planning theory call into question its validity in China. The theory emphasizes consensus building in which various stakeholders come together for dialogue to address controversial issues. It rests on three assumptions: democratic institutions, neutral power and communicative rationality. These assumptions, which are often debated in the Western context, should clearly be questioned in the Chinese context, due to authoritarian institutions and the challenging nature of power relations. Therefore, the aim of my project is to examine the practices of collaborative planning in China and identify the challenges to the assumptions of the theory. I will develop three novel tracks for examination and reconceptualization. The first will analyze how Chinese political and planning systems, social capital and culture affect the interactive processes. The second will apply network theory and social network analysis to analyze various types of power relations between government, planners, civil society and citizens. The third will identify various forms of online public spheres and how they interact with offline public spheres to affect communicative and agonistic approaches to collaborative planning. The research will employ an innovative mixed methods approach combining critical discourse analysis, data mining, computer-assisted content analysis, and social network analysis to research a wide range of case studies. My project will lead to a new understanding of collaborative planning in China, and a reconceptualization of the collaborative planning theory to make it suitable for authoritarian contexts.","CONDJUST will create a new research field, Conservation Data Justice, that bridges three distinct areas of enquiry: conservation prioritisation, political ecology and Data Justice. The former uses data which risk marginalising rural peoples. The latter does not yet examine conservation data. Meanwhile political ecologists do not yet consider Data Justice approaches when tackling conservation prioritisation. CONDJUST will interrogate conservation data and models, and explore the epistemic communities producing them, to develop new theories of socially just, data-driven conservation. It will challenge the colonising tendencies of prioritisation work and seek decolonising alternatives. CONDJUST is timely because ambitious new global targets seek to safeguard 30% of the planet for conservation by 2030 (and more afterwards). These plans pose risks for rural people because the data and modelling they use can contain diverse forms of bias, exclusion and omission. These risks will grow as more social media data are used in conservation prioritisation. We need insights from Data Justice to understand these dangers, and how they might be counter-acted. This project has four objectives, each with a corresponding work package. These are: 1. Systematically examine the sources of bias and distortion in conservation data used in global prioritisation work. 2. Use Data Justice thinking in new analyses of biodiversity conservation, and increase our understanding of socially just conservation prioritisation. 3. Critically explore the construction of different epistemic communities in conservation prioritisation, and political ecology, to understand what inhibits and enhances learning between them. 4. Examine how policies responding to prioritisation are shaped by, or resist, the new measures proposed. These work packages will be pursued by an interdisciplinary team led by the PI and composed of three post-doctoral researchers, two PhDs, an administrator and an advisory board.","Planning and property data are the key evidence base for how cities are understood, planned and developed, informing public perception, guiding investments, and shaping policy. Yet, little critical attention has been paid to planning and property data and their lifecycles, circulation, politics, power and use in policy and stakeholder decision-making. This lacuna raises two important challenges that require redress if the validity of analysis, interpretation and decision-making is to be improved. First, transforming the ontological and epistemological understanding of planning and property data amongst those that utilise them. Second, fostering a reflexive approach to data politics and power in organisations that produce, share and use planning and property data. DATASTORIES will tackle these challenges by conducting research in a creative, highly engaged way with key stakeholders across three domains (state, business, NGOs\/civil society). It will develop an innovative methodological approach that blends social science and research-creation methods, working with creative writers and artists, to map an entire data ecosystem (Dublin, Ireland), unpack data assemblages and produce a variety of data stories. 12 in-depth case studies will produce 36 data stories about and with planning and property data. The project will produce four key advances: new knowledge about the evidence-base for planning and property and its use; critical insight into the politics and praxes of data; novel research-creation methods and an assessment of their efficacy; and an extended conception of data stories and an understanding of their production and utility for different audiences. DATASTORIES will produce three ground-breaking impacts: conceptual \u2013 transforming the epistemology of planning and property research; applied \u2013 positively influencing the data processes and practices of key stakeholders; methodological \u2013 validating research-creation and data stories as social sciences methods."]},{"Topic":16,"KeyBERT":["biopsychosocial","psychosocial","phenomenological","microbiome","psychological","psychology","sociocultural","interventions","research","healthxcross"],"LLM":["Health trajectories of diverse individuals in various contexts","","","","","","","","",""],"Representative_Docs":["Videogames have become one of the most prevalent forms of cultural production around the world. While their role in teaching and physical culture (\u201cesports\u201d) keeps growing, the health debates on videogame play, or gaming, culminated in 2019 with the World Health Organization\u2019s historical decision to add \u201cgaming disorder\u201d to the International Classification of Diseases. This made gaming, next to gambling, the first and only cultural product with a diagnostic category of addictive use. The above echoes a greater conflict between culture and human development: how can science address potential problems in intensive technology use, when intensive use is also globally integrated into healthy everyday living? To build a foundation for answering this question, I pursue a Meta-Phenomenological Taxonomy of intensive gaming on three levels of lived experience: play, health, and design interaction. The taxonomy is \u201cmeta-phenomenological\u201d in the sense that it is structured on the experiences of intensively gaming individuals. These experiences surface in distinct sociocultural contexts in interaction with specific videogame designs, which are the studied meta-areas. This interdisciplinary project is cross-cultural, longitudinal, and qualitative. Participants with and without health problems (n=240) will be followed for three years in South Korea, Slovakia, and Finland. In collaboration with clinical experts, phenomenological interviews are carried out with diaries that include gaming activity logs. The design structures of the videogames in the participants\u2019 lives are analyzed to map out the phenomenological forest of health and play with specific design interactions. The elements are refined into a taxonomy that not only serves as a new foundation for \u201cgaming disorders\u201d but also situates such instances in the colorful spectrum of diverse lives and designs at large\u2014providing grounds for sustainable future theory development at the intersection of health, culture, and design.","Microbiome science is popularizing a symbiotic understanding of health and ecology. What microbiome science now knows is that microbes entangle the health of people and environments; what we don\u2019t know is how, in this process, new cultural concepts and practices of health may emerge. This project asks: how does health come to be reconfigured in a world entangled through microbial data? HealthXCross is a multi-sited, comparative ethnographic study of how scientists produce and coordinate knowledge within the Earth Microbiome Project (EMP). EMP is a US-founded and transnational research network to enable the collection, comparison and integration of microbial data across time, space and species in order to produce simulations for intervening in both environmental and human health. HealthXCross is an ethnographic inquiry into the implications of the environment being understood as a body - and viceversa - through the analysis of the tensions between the emancipatory and the dystopian effects of dissolving boundaries between human bodies and environments. With this aim, my project will examine 1) how the technology employed in EMP remakes notions of biological diversity by crossing conventional categorizations (space, time, species), 2) how the disruption of standard knowledge is performed as innovation value by making diverse epistemic cultures work together and 3) how these knowledge-making practices shape new trends in healthcare. HealthXCross will create a participatory design with scientists, who are among stakeholders in the public discourse about what it means to be human and how to live in an entangled planet. My project will offer timely insights into the interplay between knowledge making and the shaping of health practices in times of profound ecological, socio-technical and economic transition. HealthXCross will dramatically advance anthropological understandings of the contradictory but constitutive aspects of living together and being in relation.","Transgender health is an area of increasing focus for researchers and medical practitioners across Europe, in part due to rapidly escalating numbers of people identifying as transgender. A crucial oversight in this area is that very little research has examined how gender-affirming hormone therapy, the most common form of medical intervention for transgender people, shapes psychosocial functioning and ultimately social relationship experiences. Given the paramount importance of social relationships to health and well-being, evidence for psychosocial effects of gender-affirming hormone therapy is vital to ensuring health equity for transgender people, who suffer from alarmingly high rates of social disruption and suicide risk. I draw together independent strands of research on biological effects of hormones from social neuroendocrinology and health influences of psychological and sociocultural factors from social psychology and epidemiology to propose a novel biopsychosocial model linking gender-affirming hormone therapy to psychosocial functioning in transgender people. Drawing upon this model, I propose a programme of research involving four complementary work packages, triangulating across a variety of novel and cutting-edge research methods. This work will be guided by three key aims: A) to isolate causal pathways from a biopsychosocial model linking gender-affirming hormone therapy to psychosocial functioning, B) to empower transgender people to give voice to their own personal and relational experiences in the context of gender-affirming hormone therapy, and, C) to guide policy and practice for gender identity services in line with the informed consent model, directing focus to the improvement of psychosocial functioning and social relationship outcomes. This interdisciplinary programme of research will sit at the forefront of gender-affirming healthcare and treatment for transgender people."]},{"Topic":17,"KeyBERT":["nanoswimmers","chromophores","mechanochromic","mechanochemistry","polymers","biomolecular","covalent","luminescence","materials","spectroscopy"],"LLM":["Nanoparticles, Photoactivated Nanoswimmers, X","","","","","","","","",""],"Representative_Docs":["X-ray photons carry sufficient energy to interact with molecular core-shell\u2019s electrons. Accessible for decades in the energy domain, the resulting core-excited states (CES) can now be observed in the time domain using attosecond (10-18 s) spectroscopy. These states are important as they govern the lineshapes in all x-ray spectroscopies. Here, we propose to first investigate and then manipulate the CES time evolution in solvated biomolecules in order to reveal key chemical information \u2013 i.e. solute-solvent interactions, local symmetries and chiral fields. CES lifetimes dictate the emission of secondary electrons active in radiotherapy. By observing the effect of solute-solvent interactions on CES we will be able to achieve a better understanding of the first molecular mechanisms of radiotherapy. CES are also a subtle probe of the absorbing atom\u2019s bonding environment. CES line splittings are lost in conventional x-ray spectroscopy due to homogenous broadening. We developed a technique based on the laser manipulation of CES capable of producing lineshapes up to an order of magnitude below the spectroscopy\u2019s lifetime broadening and revealing core-level splitting. We will employ this approach to observe core-level splitting in solvated amino acids and metalloproteins and will use this new information to reveal the binding geometry of ligands with unprecedented accuracy. Finally, we will show how one can use nonlinear optics with attosecond pulses to reveal the chirality of the field surrounding sulphur and phosphorus atoms in biological samples. X-ray excitation localizes the point of view on the chiral field to a single atom. This perspective will allow us to examine the chiral landscape near the target atom. Here, chirality due to a single chiral centre will be probed in L-cysteine while the chirality due to the macromolecular arrangement will be measured in DNA helixes. Our proposal brings attoscience techniques in the investigation field of large solvated systems.","Stimuli-responsive polymers adapt their properties in response to external cues. Engineering such \u201csmart\u201d behaviour in artificial systems by molecular design is an exciting fundamental challenge that can lead to technological breakthroughs. Most stimuli-responsive polymers rely on heat and light to trigger changes in materials properties in a predictable fashion. However, limitations intrinsic to these stimuli highlight the necessity of alternative strategies. Naturally evolved systems widely exploit mechanical stimulation to regulate their functions, but recreating such concept in artificial materials has proven extremely challenging thus far. ReHuse proposes a radically new approach that focuses on the application of mechanical force to induce changes in bulk materials properties isothermally and reversibly. The research project aims at pushing the frontiers of covalent mechanochemistry through the development of reversible heterolytic mechanophores \u2013molecular platforms that dynamically generate and recombine two oppositely charged (macro)molecular fragments upon mechanical stimulation. These new motifs will enable dynamic chemistries involving organic ionic species in solid-state systems in two different types of advanced bulk materials. Combining reversible mechanochemistry and dynamic covalent chemistry will lead to dynamic covalent polymers displaying selective mechanoresponsiveness. This concept will be leveraged to create recyclable materials. The reversible generation of charges from the heterolytic scission will enable to modulate hydrophilicity\/hydrophobicity dynamically. Such principles will be explored to set the groundwork for mechano-responsive atmospheric water harvesters. This interdisciplinary research project will advance our understanding of mechanochemistry and, more importantly, will usher new avenues for its productive and repeatable use in adaptive materials.","The MaMa project aims at developing integrated computational approaches enabling to describe and design new mechanochromic materials. In particular, we aim at rationally design technologically consistent commodity polymers with smart and intelligent features such as mechano-responsive elastomeric, thermoplastic and thermoset polymers by the introduction of chromogenic species in the polymeric matrix in the form of mechanophore chromophores. The inter- or intra-molecular interactions ruling the opto-mechanical behaviour of the chromophores within the polymer matrix, and thereby the macroscopically perceived colour, can be tuned by application of an external force and they will be the key issue to control - by suitable design of the chromophores and polymers- and to predict by theoretical tools. In this respect the present project concerns, beside primary fundamental issues, such as the development of theoretical approaches for the description of photophysical processes in complex matrix, the handling, description and prediction of phenomena occurring from molecular to nano-scale in presence of an external stimulus such as pression\/shearing or drawing. We will focus on mechanochromic luminogenic materials based on strain-induced modulation of molecular covalent or non-covalent interactions, affecting phenomena like excimer modulation or aggregation-induced emission at the molecular chromogenic scale. The functionalized smart materials designed can find real-life applications ranging from anti-counterfeiting systems for intelligent packaging, smart coatings and textiles, and optical indicators for the detections of cracks and fatigue issues in thermoset polymers. The solid and active collaboration with leading experimental and theoretical chemists will allow for the efficient synthesis and characterization of the most promising demonstrators, to exhaustively validate the computational tools and to further illustrate the technological relevance of these materials."]},{"Topic":18,"KeyBERT":["volcanism","geodynamic","co2","tectonic","crustal","ocean","glacial","seismic","hydrothermal","sediments"],"LLM":["Wave-Breaking Research and Climate Study \n\nQ:","","","","","","","","",""],"Representative_Docs":["Green2Ice will investigate the deepest and oldest ice and basal sediments drilled from the Greenland Ice Sheet (GrIS). Ice cores have been drilled the last 55 years, but the deepest ice containing basal materials has been preserved until now, and still holds undeciphered paleoclimatic messages. The breakthrough of Green2Ice is to develop and apply cutting edge dating methods on this unique sample collection and hence to reconstruct the age and the stability of the GrIS. A hypothesis to test is if the present GrIS formed at the time of the Mid Pleistocene Transition, 1,2 - 0.8 million years ago. One innovation of Green2Ice is to gain paleo-information of the past size of the GrIS to constrain future tipping points. This knowledge will reduce the uncertainty on estimates of future sea level rise. Green2Ice will bring together four PIs from three world leading institutions with complementary skills to lift this strongly interdisciplinary program. We will drill a replicate core at GRIP, to supplement the available material from five existing ice cores and ensure retrieval of sediments and rock material from beneath the GrIS summit. We will develop, improve and apply novel dating techniques (cosmogenic and radiogenic nuclides, OSL\/IRSL, modeling of gas and isotope diffusion) to place constraints on past waxing and waning of the GrIS. State-of-the-art methodologies on fossil remains, organic matter, in situ produced and consumed greenhouse gases, and ancient bio-molecules will provide insights on the types of ecosystems and environmental conditions that emerged during ice-free conditions. Interpretation will include ice sheet modelling with data benchmarking to establish the climatic sensitivity of the GrIS. Earth system modelling and collaboration with the groups preparing the IPCC AR7 will bring the knowledge of the past into the future. The rare and unique basal ice and material can only be used once. This is the main high risk high gain component of Green2Ice.","Geological processes governed paleo-atmospheric CO2 variations and exerted major control on past climate change beyond the million-year time scale. Vast deep carbon reservoirs are known to be activated at continental rifts, where the faulted lithosphere provides CO2 pathways and where recent surveys detected massive CO2 emissions. However, progress in quantifying natural CO2 degassing and its impact on past climate is impeded for 3 reasons: (1) current CO2 flux measurement techniques require labourintensive field surveys that can cover only small areas; (2) a consistent framework uniting geodynamic processes and CO2 transport to the surface is missing; (3) past CO2 flux from rifts is difficult to quantify because compilations do not account for geodynamic characteristics. EMERGE will enter uncharted territory by linking 3 innovative approaches. The project will: (1) advance airborne CO2 flux measurements via drones. Focusing on rifts in Kenya, Ethiopia, Czech Republic and Iceland, we will measure for the first time tectonic CO2 flux distributions of entire regions allowing unprecedented insight into subsurface CO2 pathways; (2) characterise geodynamic controls on lithospheric CO2 transport via novel numerical modelling techniques; and (3) integrate data of all known rifts since 540 million years ago to understand the role of tectonic degassing in shaping Earth\u2019s climate through time. Zooming in on the geosphere-atmosphere interface, this project integrates interdisciplinary ideas and methods from geodynamics, micrometeorology, petrology, and paleoclimatology. EMERGE may generate broad impact on scientific and societal level: dronebased CO2 flux measurements will be a game changer in understanding tectonic CO2 release at rifts and other plate boundaries worldwide. The methodological and scientific advances may be essential for establishing a solid baseline of tectonic CO2 emissions to accurately quantify controls on past and future climate change.","A central problem of understanding the Earth system is quantifying climate-solid Earth feedback that requires time-series studies. One important time series is the climate (sea level) record that shows transitions between ice ages and warm periods for the last millions of years, involving vast mass transfer between continents (ice load) and oceans. Volcanism is sensitive to such pressure changes, but its response to glacial cycles is largely unknown for the global mid-ocean-ridge (MOR) system, where 80% of Earth\u2019s volcanism occurs. Models of MOR response to sealevel fluctuations predict changes in crustal thickness, chemistry of lavas and hydrothermal activity. Establishing high-resolution time series on MORs, however, has previously not been possible, because the sea floor is rapidly covered by sediment as it moves away from the MOR and thus cannot be directly sampled. Recent studies, however, show MOR eruptions deposit samples of lava as glass on nearby sediments for up to 100 ka. These carbonate-rich sediments can be precisely dated by oxygen isotope stratigraphy and provide an archive of ridge eruptions (glasses) and hydrothermal activity (trace metals) in the sediments that can be sampled by gravity coring. Through closely spaced new cores to be retrieved during multiple research cruises, a high-resolution time series of volcanism and hydrothermal activity can be achieved and directly linked to the climate record, whereas seismic techniques can be used to determine variations in crustal thickness over time. We propose to obtain integrated data sets for all these processes from slow, intermediate and fast spreading ridge segments over the past 1.5 Ma in unprecedented detail. The results of these glass, sediment and crustal thickness time series will allow us to unequivocally test the influence of glacial cycles on MOR processes and will provide the first high-resolution time series of ocean ridge magmatism, opening up a new frontier of scientific exploration."]},{"Topic":19,"KeyBERT":["nanobots","microrobots","nanoswarms","nanobot","swarms","nanoparticles","swarm","biological","bioenergy","oscillators"],"LLM":["Nano-Communication and Emerging Agents\n\nThe rationale","","","","","","","","",""],"Representative_Docs":["In nature, systems composed of self-propelling agents display complex behaviors such as signal interpretation, propagation, amplification and engage in collective motion mediated by interactions between different agents and their environment. Examples range from swarming bacteria to schooling fish and flocking birds. These self-organized systems have served as an inspiration for researchers seeking to achieve complexity in artificial systems composed of synthetic agents. A class of agents that has recently been demonstrated is of synthetic nanomachines (nanobots) that can self-propel thanks to the conversion of chemical energy, harvested from the environment, into motion. While most of the artificial nanobots have been explored at individual level, their collective emergent behavior, arising from inter-particle interactions through chemical and hydrodynamic fields, and through environment mediated interactions is yet to be properly studied. Understanding collective effects will be especially useful in biologically relevant environments, where a number of applications for these nanobot systems have been envisioned. i-NANOSWARMS aims to realize enzyme-powered nanobot swarms capable to self-propel using biocompatible and bioavailable fuels and display collective and cooperative behaviours through communication among them as well as with the host environment. The proposal is divided in three working packages. In WP1, I will create a toolbox of nanobots based on a library of enzymes and nanoparticle architectures to study communication and long-range signal propagation using enzyme cascades. WP2 will be devoted to the collective behavior of nanobot swarms, exploiting biomimetic strategies such as chemotaxis and stigmergy to guide and recruit other nanobots. WP3 aims at studying, as a proof-of-concept of the applicability of intelligent nanoswarms for biomedical applications, cooperative behavior among nanoswarms for enhanced drug delivery and medical imaging.","This project aims to the development of communication at the nanoscale and to advance in the understanding of how abiotic micro\/nanoparticles can communicate between them and how micro\/nanoparticles can communicate with living systems. In this context, an approach for establishing communication at the nanometric level is to mimic how nature communicates. Chemical or molecular communication, based on transmitting and receiving information by means of molecules (chemical messengers) is one of the communication forms used by living organisms. Moreover, many swarm systems found in nature communicate by modifying the environment using a concept called stigmergy. The advantages of nanoparticles that communicate each to another are immediately obvious; they constitute the basis of a dynamically interacting network eventually resulting in certain autonomy of the system. If we would be able to raise the bases for communication between micro\/nanoparticles and between micro\/nanoparticles and cells, the potential future applications in the biomedical field, environmental research and industry technology are almost unlimited. The project will establish firm handholds for the use of nanoparticles able to communicate from one to another and with cells in different applications. The project will trace, optimise and adapt all single steps from the idea to its implementation into applicable final systems with the aim of targeting issues that are difficult to address with conventional single particles. The project is divided into three WPs. The first work package (WP1) will create the basic elements for chemical communication. In a more complex situation, WP2 will use the tools of WP1 to develop systems able to establish communication between nanoparticles and living systems. Finally, WP3 will generate nano-systems integrating gated nanoparticles and up-to-date electronics to develop new communication structures.","ONLINE aims to develop new concepts of communication between inanimate materials. What is meant by communication? In biological context, communication refers to interactive behaviour of one organism affecting the current or future behaviour of another. In the context of bioinspired materials, ONLINE will develop life-like material structures that communicate with each other via physical contact, fluidic medium, or optical beams. These inanimate materials will be coupled to form networks that communicate autonomously through light. How to make them? The core concept behind the communicative materials is self-oscillatory (self-sustained) motions in light-responsive liquid crystal elastomers (LCEs). I have recently discovered that (I) an individual LCE self-oscillator can react to disturbances spontaneously, indicating the ability to interact with surrounding environment; (II) The coupling between individual oscillators can be attained through multiple laser beams, which enables material communication \u2013 if one oscillator is disturbed, the whole network percepts. My goal is to scale down these proof-of-principle concepts to the micro-scale and realize soft material robots that can communicate. Why is this important? There exists an increasing need for artificial materials that can interact, alike biological systems. However, all the dynamic features of state-of-the-art responsive materials are based on internal material properties, and making individual materials interact with each other is a huge challenge. ONLINE proposes three new model systems for material communication: (I) Microscopic walker swarm, in which the locomotion and patterns of interactions between individuals can be fully programmed; (II) Cilia array that move cooperatively and self-regulate the fluidics at low Reynolds numbers; (III) Homeostasis-like light-communicating coupled network that provides a full set of tunable parameters to mimic the complexity of biological oscillators."]},{"Topic":20,"KeyBERT":["palaeoproteomics","neolithic","hominins","archaeological","socioecological","hominid","ancient","evolution","subsistence","specimens"],"LLM":["Molecular anthropology and ancient DNA sequencing in Eurasia'","","","","","","","","",""],"Representative_Docs":["The intimate relationship between mother and infant, since the first moments after conception, contributed to shaping the evolution of our species and its behaviour. The mother-infant nexus changed through time adapting to environmental changes, new subsistence economies and social constraints. Yet, how the biocultural transitions across human evolution influenced the mode and time of pregnancy and nursing of human infants is under-investigated. MOTHERS will use recently developed cutting edge methodologies of trace elemental and isotopic analyses in dental enamel and dentine to identify the change from an exclusive breast milk diet to one that includes non-milk foods and to assess the mother\u2019s diet and well being. Indeed, dietary behaviour, including that of the mother during pregnancy, deeply affects human growth and development from the earliest phases of ontogenesis and is chemically recorded in developing dental enamel. The goal of MOTHERS is to build consistent interpretative models, based on contemporary infants with controlled dietary and anamnestic history, to reconstruct health, diet, and growth trajectories in early life on an extensive collection of human dental specimens from the Upper Palaeolithic to Neolithic, until urbanization, in Italy and Croatia. Also, the profound chemical differences in dental enamel between breastfed and herbivore milk\/formula-fed children will allow the identification of the early use of non-human milk and shed light on the herbivore domestication and on alloparental care in past human populations. This project adds value and competitiveness to the bioarchaeological research landscape in Europe. Not only will my project be of interest to a broad range of academics within the social sciences, but it can inform present-day public health policy measuring the effect of dietary shifts in children\u2019s growth and development.","The prehistoric pile dwellings in and around the Alps constitute one of the most important archaeological archives of human prehistory. Dating from around 5000 to 500 BC, there are over 1000 known sites in the region, 111 of which are listed on UNESCO\u2019s World Heritage List. The sites are mainly located under water, on lake shores, along rivers, or in wetlands, offering exceptional conditions for the preservation of organic materials like wood, plant remains, animal bones, artefacts, and even textiles. Because of their exceptional preservation, the archaeological remains from those sites give us a unique window into the lives of prehistoric people and the development of early agrarian societies in Central Europe. However, despite the rich material evidence from the settlements, we know relatively little about the people who lived there. This is because there are no burials directly associated with the lake settlements, which has precluded the study of ancient DNA, for example. Luckily, there are other sources of ancient DNA, including ancient \u201cchewing gums\u201d which provide a rich of ancient human and host-associated microbial DNA as we recently demonstrated. In this project we will sequence ancient DNA and other biomolecules from ancient \u201cchewing gums\u201d found at lake settlements in and around the Alps to shed new light on the lives of the Alpine communities that settled there between 5000 and 500 BC. With access to over 300 specimens from archaeological sites north and south of the Alps, we have the unique opportunity to study their interactions and the demographic and cultural changes that characterised the transition from the Neolithic to the Bronze Age in Central Europe. In addition, the project promises to offer new insights into peoples\u2019 health and the composition of their oral microbiome, as well as their diet and subsistence strategies. Together, the proposed research will provide us with a richer understanding of the pile-dwelling communities of Central Europe.","The project aims to question human migrations and peopling: why the earliest hominins did occupy Western Europe later than other portions of Eurasia? Early Hominins, conquered Eurasia, long before Modern Humans, the single Homo species living now on the earth. They conquered Eurasia along a rapid \u201cOut of Africa\u201d movement but left Europe empty during almost 1 million years. Western Europe did indeed face environmental constraints. This subcontinent is in a remote corner of Eurasia but other large Eurasian peninsulas are dead-ends as well. Certainly, Western Europe mixture of various environments and topographies did change a lot over time and the succession of climates, then causing favorable territories for human occupations to fluctuate. However, archaeology in Asia and the Levant shows that hominins did overcome variable climatic conditions and geographies. Investigating why Western Europe have remained out of the \u201cOekoumen\u201d for so long is our research proposal LATEUROPE, that we base over datasets of interdisciplinary and behavioral materials enriched by future fieldworks in several specific sites and biomes on the key period before 500 ka. This will question at the local, regional and continental scale, the environmental, geographic and climatic conditions of Europe as compared to the rest of Eurasia and the characteristics of the hominin occupations and behaviours, or\/and if a minimum degree of cognition was required to thrive in these lands. Databases will input multiple scenarios combining migration patterns and internal evolution mechanisms, using conceptual modeling and spatial agent-based simulations. This formalized combination of modeling and field methodologies is an epistemological advance for bringing interdisciplinarity to reality, allowing us to deeply question the ability of Homo species to adapt themselves to harsh environments, to face environmental shocks and changes on a long term scale."]},{"Topic":21,"KeyBERT":["sdgs","sustainability","sustainable","dynamics","2030","research","diffusion","beyondsdg","dispersion","growth"],"LLM":["Sustainability Transitions and Price Dispersion","","","","","","","","",""],"Representative_Docs":["Staying within a maximum global warming of 1.5 degrees requires an acceleration of the transition to a low carbon society. Current theory views such sustainability transitions as inevitably slow as they require interdependent changes throughout socio-technical systems. The observation that energy transition scenarios systematically underestimate the diffusion of key technologies challenges this view. Existing models that form the basis for energy transition scenarios do not include the positive feedbacks resulting from the interactions between civil society and energy transitions. Yet these interactions have been identified as important drivers and barriers of energy transitions, and as crucial in triggering the social tipping dynamics that can accelerate the energy transition. Social tipping dynamics in energy transitions occur when a small change or intervention has a large effect on emission reductions. To date, some examples of social tipping dynamics have been identified, but both a systematic understanding of the mechanisms underlying social tipping dynamics and an evaluation of their potential policy leverage is missing. The overall objective of the FAST project is to explain and model social tipping dynamics and interventions in energy transitions. This requires a model that explains how the social factors influencing sustainability transitions scale up to realize social tipping dynamics. To this end I will use a novel combination of bottom-up agent-based modelling and top-down diffusion modelling to capture the interactions in socio-technical systems that create tipping dynamics. This will bridge the qualitative field of sustainability transitions and the quantitative field of energy transitions modelling. A team consisting of the PI, 3 PhD students, a postdoc and a research assistant will conduct the proposed study over five years. This is the first study that systematically integrates social tipping dynamics in quantitative models of energy transitions.","How can we sustain human well-being within planetary boundaries? What policies and provisioning systems could enable societies to prosper without growth? What politics and alliances are necessary for seeing post-growth policies through, and how can the public be engaged in them? What new scientific paradigm could answer such questions? Societies face multiple intertwined crises. Bold alternatives are sorely needed. This project develops frameworks for \u2018Post-Growth Deals\u2019, from empirical research through to practical applications. First, we develop equitable North-South convergence scenarios, modelling human well-being achievement in all countries within planetary boundaries. Second, we articulate post-growth policy packages for the Global North and South, assessing their political acceptability and modelling their effects. Third, we develop models of provisioning systems to ensure future populations have adequate energy, food, shelter, health and social security. Fourth, we learn from political movements, studying politics and alliances that could bring post-growth transitions forward. Fifth, we identify practical steps to bring Post-Growth Deals to life, working with four representative communities to co-produce knowledge and action on the ground. The potential gains of this research are immense: post-growth transitions may unlock a far more ecologically stable and socially prosperous future than current trajectories lead to. REAL brings a paradigm shift moving post-growth science from economics to sustainability studies. We propose a new trans-disciplinary \u20185Ps of post-growth\u2019 science, grounded in resource\/energy modelling, political-economy and socio-political analysis \u2013 a skill-set that no single researcher or team presently possesses. The PIs are leaders in their fields and bring complementary expertise in: modelling of provisioning systems (JST), political economy and North-South relations (J\u0397), and the politics of socio-environmental transformations (GK).","Countries are not on track to meet the 2030 Agenda for Sustainable Development that comprises 17 Sustainable Development Goals (SDGs) and 169 targets to be achieved by 2030. Although SDGs aim to shift the world onto a sustainable and resilient path, countries are not yet able to make transformative changes for long-term sustainability that requires building social prosperity and foundations within planetary boundaries. Failing to achieve SDGs will negatively affect billions of people and worsen environmental conditions and socio-economic problems. Therefore, BeyondSDG aims to understand the necessary conditions for long-term sustainability, including achieving SDGs, based on the following specific objectives: i) identify critical targets for prioritising SDGs; ii) investigate the effects of (under)achieving SDGs on long-term sustainability beyond 2030; and iii) identify sustainability targets for the post-2030 development agenda. For this, BeyondSDG will apply a threefold scientific approach that combines statistical analysis of empirical and modelled data, qualitative analysis of literature, and knowledge co-creation with stakeholders, including sectoral experts and policymakers, based on systems thinking. This combination of three approaches is complementary and essential to deal with the complex topic of long-term sustainability. Consequently, BeyondSDG will lead to a breakthrough in interdisciplinary research by combining approaches from sustainability science, earth system modelling, and environment and resource management. Mainly, it will bring the following ground-breaking findings: critical targets where adequate actions can lead to progress across most SDGs, negative impacts of underachieving SDGs on people, the planet, and prosperity, required extra efforts for long-term sustainability besides achieving SDGs, and sustainability targets for the post-2030 Agenda based on lessons learned from SDGs, state-of-the-art science, and stakeholder partnerships."]},{"Topic":22,"KeyBERT":["microstructure","microstructures","multiscale","materials","dislocations","nanometers","dislocation","femtosecond","ceramics","structural"],"LLM":["Heterogeneity engineering \nB:\nC:\nD","","","","","","","","",""],"Representative_Docs":["Over the last decade, the steadily increasing research on gradient-structured metals and alloys has demonstrated great successes of this biological and nature-inspired concept to evade the strength-ductility trade-off dictating in regu-lar engineering materials. However, given the intrinsic limitations of conventional manufacturing methods, the currently engineered structural gradient materials are all featuring a linear pattern, usually from exterior to interior, with only simple structures of grain sizes, twin spaces, lamellae spaces, or combinations. A recent preliminary study of my research team accidentally discovered that additive manufacturing could produce periodic and 3D gradient microstructures with not only simple microstructure features, but also hierarchical ones, from grain size to sub-grain boundaries and even lattice distortions. The hierarchical gradient microstructure keeps the same level of high strength but doubles the failure strain compared to the conventional microstructures. This inspires us to systematically investigate the possibilities and boundaries of these new hierarchical gradient microstructures by additive manufacturing. Due to its extreme complexity across multiple scales and physics laws in correlating process, microstructure, and property of such new materials, we aim to develop a systematic approach for designing hierarchical gradient microstructures by using design of experiments, in-depth and multiscale characterization methods, multiphysics and multiscale numerical models, and data informatics. The intelligent integration of the physics-based and data-driven models will eventually boost the dimensionality, efficiency, and accuracy of the modeling approach for the design of the complicated hierarchical gradient microstructure in 3D. It will provide a powerful, digital and sustainable way for the design of new materials and\/or processes and evaluation of material performance.","Advanced functional ceramics play an indispensable role in our modern society and they are typically engineered by point defects or interfaces. The potential of dislocations (one-dimensional atomic distortions) in functional ceramics has been greatly underestimated until most recently. Exciting proofs-of-concept have been demonstrated for dislocation-tuned functionality such as electrical conductivity, superconductivity, and ferroelectric properties, revealing a new horizon of dislocation technology in ceramics for a wide range of next-generation applications from sensors, actuators to energy converters. However, it is widely known that ceramics are hard (difficult to deform) and brittle (easy to fracture), making it a great challenge to tailor dislocations in ceramics. This pressing bottleneck hinders the dislocation-tuned functionality and the true realization of dislocation technology. To break through this bottleneck, MECERDIS employs mechanics-guided design coupled with external fields (thermal, light illumination, electric field) to manipulate the 3 most fundamental factors of dislocation mechanics: nucleation, multiplication, and motion. These external fields greatly impact the charged dislocation cores in ceramics and open new routes for mechanical tuning. With these novel approaches, MECERDIS aims to generate, control, and stabilize dislocations in large plastic volumes up to mm-size with high density up to 10^15\/m^2 to allow large-scale preparation for functionality assessment. Another essential benefit is, dislocations are an effective tool to combat the brittleness of ceramics by improving the damage tolerance and fracture toughness. MECERDIS will not only fulfil the key prerequisite of dislocation-tuned functionality but also secure the mechanical integrity and operational stability of future dislocation-based devices. With its success, MECERDIS will define a new paradigm of engineering functional ceramics using mechanics and dislocations.","Oxide glasses are one of the most important material families owing to their unique features, such as transparency, tunable properties, and formability. Emerging solutions to major global challenges related to energy, health, and electronics require new scientific breakthroughs in glass chemistry, mechanics, and processing. The realization of these goals is severely restricted by the main drawback of glass, namely high brittleness. Furthermore, new glass compositions are today developed through time-consuming trial-and-error experimentation due to their inherent non-equilibrium nature and disordered structure. A major task is therefore to initiate a paradigm shift within the field of glass science and technology, going from empirical to model-based approaches for the design of new glass compositions and microstructures with improved fracture resistance. This requires the development of computational approaches, from ab initio calculations to artificial intelligence, to integrate structural descriptors and glass chemistry with advanced processing and mechanical properties into holistic tools. NewGLASS challenges the current glass design strategies in order to create such tools. For this purpose, an interdisciplinary approach is proposed, in which structural descriptors at the short- and medium-range length scales are first identified and quantified based on emergent statistical mechanics and persistent homology techniques. Guided by these results, high-throughput simulations at various length scales are combined with machine learning algorithms to design novel glass compositions, tailored deformation mechanisms, and 3D-printed microstructures to achieve superior fracture resistance. By having experiments and modelling complement and advance each other reciprocally, NewGLASS will find order in disorder and provide the scientific breakthroughs for the accelerated design of glasses with outstanding mechanical performance, thus opening up for many new applications."]},{"Topic":23,"KeyBERT":["sustainability","sustainable","infrastructures","2050","adaptation","resilient","climate","impact","forecasting","research"],"LLM":["Urban Systems Resilience\nQ:\nWhat are the main research","","","","","","","","",""],"Representative_Docs":["urbisphere will change how the scientific community conceptualises, characterizes and forecasts cities in the climate system and in urban planning, by developing a radically new approach to integrate multiple dimensions of urban change, their interaction and feedbacks. It aims to forecast and project urban futures and climates in a dynamic framework considering weather, air quality, differential exposure and vulnerability of people at neighbourhood to city scale. It will provide new insights into existing and emerging risks, based on a synergistic effort across disciplines which currently work mostly in parallel. Urban-Surface Models (USM) and Human Exposure and Vulnerability models (HEV) will be developed and coupled to improve the forecasting of exposure, emissions, and intervention potentials in cities. This will transform emergency\/risk management, atmospheric forecasting and long-term urban development\/adaptation strategies in the urban sphere. The system will use a real-time 4D Smart Urban Observation System (SmUrObS) to provide targeted urban form\/function\/emissions\/exposure data using novel ground and remote sensing technology. The USM-HEV-SmUrObS system will equip us with: 1) a deep understanding of socio-economic dynamics and human behaviour and responses to weather and climate, economic (and other) drivers that transform cities\u2019 exposure and vulnerability to climate change-related hazards (like heat); 2) a consistent method that can be scaled from detailed high-resolution modelling of intra-neighbourhood scale characteristics, to climate and socio-economic modelling and assessment at city, regional and global scales; 3) an approach that can inform global climate and global vulnerability and risk modelling; will allow consistent downscaling to the city for decision making for local urban risk and resilience management; and provide information on the dynamic nexus of exposure and vulnerability of people in cities.","The world population living in urban settlements is expected to increase to 70% of 9.7 billion by 2050. Historically, as cities grew, new water infrastructures followed as needed. However, these developments had less to do with real planning than with reacting to crisis situations and urgent needs, due to the inability of urban water planners to consider long-term, deeply uncertain and ambiguous factors affecting urban development and water demand. These, coupled with increasingly uncertain climate conditions, indicate the need for a more holistic and intelligent decision-making framework for managing water infrastructures in the cities of the future. This project aims to develop a new theoretical framework for the allocation and development decisions on drinking water infrastructure systems, so that they are socially equitable, economically efficient and environmentally resilient, as advocated by the UN Agenda 2030, Sustainable Development Goals. The framework will integrate real-time monitoring and control with long-term robustness and flexibility-based pathway methods, and incorporate economic, social, ethical and environmental considerations for sustainable transitioning of urban water systems under deep uncertainty with multiple possible futures. The Water-Futures team will build on synergies from the four research groups, transcending methodologies from water science, systems and control theory, economics and decision science, and machine learning, into an integrated decision and control framework, to be implemented as an open-source research toolbox. The new science outcomes will be applied to three case studies exemplifying different types of urban water systems: a mature, relatively stable system; a mature and rapidly expanding system; and a relatively recent supply system in a developing country with high growth and special challenges, including limited resources, intermittent supply and high water losses.","The world population living in urban settlements is expected to increase to 70% of 9.7 billion by 2050. Historically, as cities grew, new water infrastructures followed as needed. However, these developments had less to do with real planning than with reacting to crisis situations and urgent needs, due to the inability of urban water planners to consider long-term, deeply uncertain and ambiguous factors affecting urban development and water demand. These, coupled with increasingly uncertain climate conditions, indicate the need for a more holistic and intelligent decision-making framework for managing water infrastructures in the cities of the future. This project aims to develop a new theoretical framework for the allocation and development decisions on drinking water infrastructure systems, so that they are socially equitable, economically efficient and environmentally resilient, as advocated by the UN Agenda 2030, Sustainable Development Goals. The framework will integrate real-time monitoring and control with long-term robustness and flexibility-based pathway methods, and incorporate economic, social, ethical and environmental considerations for sustainable transitioning of urban water systems under deep uncertainty with multiple possible futures. The Water-Futures team will build on synergies from the four research groups, transcending methodologies from water science, systems and control theory, economics and decision science, and machine learning, into an integrated decision and control framework, to be implemented as an open-source research toolbox. The new science outcomes will be applied to three case studies exemplifying different types of urban water systems: a mature, relatively stable system; a mature and rapidly expanding system; and a relatively recent supply system in a developing country with high growth and special challenges, including limited resources, intermittent supply and high water losses."]},{"Topic":24,"KeyBERT":["probabilistic","inference","stochastic","algorithms","generalization","statistical","algorithm","explanations","learning","models"],"LLM":["Decentralized Learning and AI Explanation\n\nThe given topic is","","","","","","","","",""],"Representative_Docs":["A wealthy friend of mine asks for a vacation credit card to his bank, to discover that the credit he is offered is very low. The bank teller cannot explain why. My stubborn friend continues his quest for explanation up to the bank executives, to discover that an algorithm lowered his credit score. Why? After a long investigation, it turns out that the reason is: bad credit by the former owner of my friend\u2019s house. Black box AI systems for automated decision making, often based on ML over (big) data, map a user\u2019s features into a class or a score without explaining why. This is problematic for lack of transparency, but also for possible biases inherited by the algorithms from human prejudices and collection artefacts hidden in the training data, which may lead to unfair or wrong decisions. I strive for solutions of the urgent challenge of how to construct meaningful explanations of opaque AI\/ML systems, introducing the local-to-global framework for black box explanation, articulated along 3 lines: a) the language for explanations in terms of expressive logic rules, with statistical and causal interpretation; b) the inference of local explanations for revealing the decision rationale for a specific case; c), the bottom-up generalization of many local explanations into simple global ones. An intertwined line of research will investigate both causal explanations, i.e., models that capture the causal relationships among the features and the decision, and mechanistic\/physical models of complex system physics, that capture the data generation mechanism behind specific deep learning models. I will also develop: an infrastructure for benchmarking, for the users' assessment of the explanations and the crowdsensing of observational decision data; an ethical-legal framework, for compliance and impact of our results on legal standards and on the \u201cright of explanation\u201d provisions of the GDPR; case studies in explanation-by-design, with a priority in health and fraud detection.","Arguably, the most crucial objective of Learning Theory is to understand the basic notion of generalization: How can a learning agent infer from a finite amount of data to the whole population? Today's learning algorithms are poorly understood from that perspective. In particular, best practices, such as using highly overparameterized models to fit relatively few data, seem to be in almost contradiction to common wisdom, and classical models of learning seem to be incapable of explaining the impressive success of such algorithms. The objective of this proposal is to understand generalization in overparameterized models and understand the role of algorithms in learning. Toward this task, I will consider two mathematical models of learning that shed light on this fundamental problem. The first model is the well-studied, yet only seemingly well-understood, model of Stochastic Convex optimization. My investigations, so far, provided a new picture that is much more complex than was previously known or assumed, regarding fundamental notions such as regularization, inductive bias as well as stability. These works show that even in this, simplistic setup of learning, understanding such fundamental principles may be a highly ambitious task. On the other hand, given the simplicity of the model, it seems that such an understanding is a prerequisite to any future model that will explain modern Machine Learning algorithms. The second model considers a modern task of synthetic data generation. Synthetic data generation serves as an ideal model to further study the tension between concepts such as generalization and memorization. Here we with a challenge to model the question of generalization, and answer fundamental questions such as: when is synthetic data original and when is it a copy of the empirical data?","Recent years have witnessed tremendous progress in the field of Machine Learning (ML). Learning algorithms are applied in an ever-increasing variety of contexts, ranging from engineering challenges such as self-driving cars all the way to societal contexts involving private data. These developments pose important challenges (i) Many of the recent breakthroughs demonstrate phenomena that lack explanations, and sometimes even contradict conventional wisdom. One main reason for this is because classical ML theory adopts a worst-case perspective which is too pessimistic to explain practical ML: in reality data is rarely worst-case, and experiments indicate that often much less data is needed than predicted by traditional theory. (ii) The increase in ML applications that involve private and sensitive data highlights the need for algorithms that handle the data responsibly. While this need has been addressed by the field of Differential Privacy (DP), the cost of privacy remains poorly understood: How much more data does private learning require, compared to learning without privacy constraints? Inspired by these challenges, our guiding question is: How much data is needed for learning? Towards answering this question we aim to develop a theory of generalization which complements the traditional theory and is better fit to model real-world learning tasks. We will base it on distribution-, data-, and algorithm-dependent perspectives. These complement the distribution-free worst-case perspective of the classical theory, and are suitable for exploiting specific properties of a given learning task. We will use this theory to study various settings, including supervised, semisupervised, interactive, and private learning. We believe that this research will advance the field in terms of efficiency, reliability, and applicability. Furthermore, our work combines ideas from various areas in computer science and mathematics; we thus expect further impact outside our field."]},{"Topic":25,"KeyBERT":["neuromodulation","connectomes","neuroimaging","neurons","stimulation","hippocampus","neural","brain","cortex","prefrontal"],"LLM":["Neuroprostheses and Neuromodulation in Hearing,","","","","","","","","",""],"Representative_Docs":["Hearing loss is the most common sensory deficit in the elderly, and it is becoming a severe social as well as a health problem. Across the whole lifespan, from new-borns to the elderly, hearing loss impairs the exchange of information, thus significantly impacting everyday life, causing loneliness, isolation, dependence, frustration and communication disorders. Cochlear implants (CIs) are hearing prosthetics that stimulate the auditory nerve with electrodes placed inside the cochlea. CIs are gradually being implanted in subjects retaining low-frequency residual hearing. In general, these subjects obtain large benefits in speech perception from electric acoustic stimulation, although large variability exists and some subjects do not benefit. Therefore, it is highly desirable to create objective diagnostics to assess acoustic low-frequency hearing to indicate cochlear implantation, to monitor and preserve hearing during the implantation procedure and to understand the mechanisms related to electric acoustic stimulation benefits. The ground-breaking nature of the READIHEAR project is to investigate the fundamental interaction mechanisms between electric and acoustic stimulation across the auditory pathway, from the cochlea up to the auditory cortex. The fundamental understanding will set the basis for a new generation of diagnostic devices of hearing loss that combine for the first time minimally invasive electric acoustic stimulation. Moreover, READIHEAR will assay a novel auditory prosthetic that makes use of the interaction mechanisms between acoustic and electric stimulation delivered through minimally invasive electrodes. These developments will be beneficial for a large population suffering from hearing loss across the whole lifespan, from young children who will benefit from improved hearing diagnostics to the elderly population who will benefit from minimally invasive electric acoustic stimulation technology as the treatment for age-related hearing loss.","Many individuals suffer partial or complete muscle paralysis with no available cures. Even though neural interfaces have the potential to restore motor function with assistive systems, their use is still very limited. Even in the case of state-of-the-art invasive neural implants, the control of the movements of the paralyzed limbs is highly unsatisfactory. These neural interfaces suffer high surgical risks, poor control of the activity of spinal motor neurons, and inaccurate mapping of the attempted movements. Spinal motor neurons are the last cells of the nervous system that convert motor commands into movement and their activity can be accessed with minimally invasive methods. In most neural lesions, such as spinal cord injury and stroke, there are functionally active spinal motor neurons projecting to paralyzed muscles that are modulated by brain input. In this project, I propose a bidirectional interface that is driven by the real-time identification of efferent spinal motor neuron activity. We will develop novel sensing, decoding, and feedback methods with precise cellular resolution. This neural interface will map, engage, and augment the spared output of the spinal cord through new deep learning methods and hundreds of fine-tuned electromyographic sensors recording action potentials of individual motor units for the muscles controlling the hand. The output of this interface will enable highly accurate temporal associations between efferent motor neuron activity and sensorimotor feedback by delivering multiple visual and somatosensory inputs. This bidirectional neural interface will entrain and monitor the spared neural pathways at the direct cellular level with the goal of transforming and augmenting the activity of the spared motor neurons into highly functional motor dimensions. Using these new technologies, we aim to answer open questions in movement neuroscience and spinal cord injury.","By contracting our muscles, we move and interact with the world. In turn, the activated muscles act as signal repeaters of the neural inputs they receive. Muscles do not only receive inputs determining how they need to contract (and thereby how we move), but they also receive a rich set of neural information originating in the central nervous system and traveling through nerves and muscles without directly altering motor commands. This 'motor null space' in the muscles may represent a unique opportunity to explore the human central nervous system in an unobtrusive, spatially selective, and robust way, thereby overcoming the most critical inherent limitations of currently available non-invasive neuroimaging technologies. To test this novel concept, fundamental research is needed to develop methods to extract, separate, and interpret the non-motor neural projections to human muscles. ECHOES will capitalize on recent breakthroughs in decoding the spinal outputs to muscles to develop a theoretical and experimental framework to unveil the 'motor null space' in human muscles. The project will then demonstrate the potential benefits of the extracted neural information in three scientific fields with growing societal and clinical impact: human-machine interfaces, targeted brain neuromodulation, and diagnosis of movement disorders. I expect that the project's multidisciplinary research program will further our understanding of the origin and relevance of neural signals generated by the human brain and spinal cord. This will markedly improve future research aimed to understand, use, and modulate human neural activity by providing a first-of-its-kind, minimally invasive, and robust neuroimaging technology with unprecedented spatio-temporal resolution. By achieving these goals, ECHOES technology will enable the development of new applications for the clinical and industry fields."]}]